#include "eyetrack.h"
#include "ui_eyetrack.h"


eyeTrack::PerformanceStats eyeTrack::performanceStats;


eyeTrack::eyeTrack(QWidget *parent) :
    QWidget(parent),
    ui(new Ui::eyeTrack)
{
    ui->setupUi(this);
    initializeComponents();
    scanCreamDevice();
    connect(ui->StarPushButton, &QPushButton::clicked, this, &eyeTrack::on_StarPushButton_clicked);
    connect(ui->OutPushButton, &QPushButton::clicked, this, &eyeTrack::on_OutPushButton_clicked);
    connect(ui->OutSavePushButton, &QPushButton::clicked, this, &eyeTrack::on_OutSavePushButton_clicked);
    connect(m_stopButton, &QPushButton::clicked, this, &eyeTrack::on_StopPushButton_clicked);
    connect(mergedPip, &MergedProcessingPip::processingComplete, this, &eyeTrack::processMergedResult);
    connect(this, &eyeTrack::chartSignals, this, [this]() {
        if (!m_trueGazePoints.empty() && !m_actualPredictions.empty()) {
            auto lastGaze = m_trueGazePoints.rbegin()->second;

            int lastFrameId = m_trueGazePoints.rbegin()->first;
            if (m_actualPredictions.find(lastFrameId) != m_actualPredictions.end()) {
                auto lastPredicted = m_actualPredictions[lastFrameId];
                chartUpdates(lastGaze, lastPredicted, lastFrameId);
            } else {
                // å¦‚æœæ²¡æœ‰é¢„æµ‹å€¼ï¼Œä½¿ç”¨çœŸå®å€¼
                chartUpdates(lastGaze, lastGaze, lastFrameId);
            }
        }
    });
}

eyeTrack::~eyeTrack()
{
    if (currentState == RUNNING) {
        qDebug() << "ææ„æ—¶å¼ºåˆ¶åœæ­¢ç³»ç»Ÿ";
        currentState = STOPPING;

        if (timer) {
            timer->stop();
        }

        if (mergedPip) {
            disconnect(mergedPip, nullptr, this, nullptr);
        }

        if (pip) {
            pip->safeDeletePipeline();
        }
    }

    delete cameraPipe;
    delete mergedPip;
    delete pip;
    delete ui;

    qDebug() << "eyeTrackææ„å®Œæˆ";
}

void eyeTrack::initializeComponents() {
    currentState = STOPPED;
    cameraFlag = false;


    performanceLabel = new QLabel("çœ¼éœ‡æ£€æµ‹ç³»ç»Ÿå·²å°±ç»ª", this);
    performanceLabel->setGeometry(10, 10, 500, 30);
    performanceLabel->setStyleSheet("color: green; font-weight: bold; background-color: rgba(0,0,0,0.1); padding: 5px;");

    // === å›¾è¡¨è®¾ç½® ===
    GazePlot = new QCustomPlot(this);
    GazePlot->setGeometry(600, 1080, 600, 300);
    GazePlot->xAxis->setLabel("ORIGINAL PLOT X");
    GazePlot->yAxis->setLabel("ORIGINAL PLOT Y");
    GazePlot->xAxis->setRange(0, 2000);
    GazePlot->yAxis->setRange(0, 1500);

    GazePointGraph = GazePlot->addGraph();
    GazePointGraph->setScatterStyle(QCPScatterStyle(QCPScatterStyle::ssCircle, QPen(Qt::blue), QBrush(Qt::blue), 5));
    GazePointGraph->setLineStyle(QCPGraph::lsNone);
    GazePlot->setInteractions(QCP::iRangeDrag | QCP::iRangeZoom);

    PredictPlot = new QCustomPlot(this);
    PredictPlot->setGeometry(1200, 1080, 600, 300);
    PredictPlot->xAxis->setLabel("Predict PLOT X");
    PredictPlot->yAxis->setLabel("Predict PLOT Y");
    PredictPlot->xAxis->setRange(0, 2000);
    PredictPlot->yAxis->setRange(0, 1500);
    PredictPointGraph = PredictPlot->addGraph();
    PredictPointGraph->setScatterStyle(QCPScatterStyle(QCPScatterStyle::ssCircle, QPen(Qt::red), QBrush(Qt::red), 5));
    PredictPointGraph->setLineStyle(QCPGraph::lsNone);
    PredictPlot->setInteractions(QCP::iRangeDrag | QCP::iRangeZoom);



    m_stopButton = new QPushButton("åœæ­¢",this);
    m_stopButton->setObjectName("stopButton");
    m_stopButton->setGeometry(QRect(1950, 1190, 171, 51));  // è°ƒæ•´Yåæ ‡ï¼Œé¿å…è¶…å‡ºçª—å£
    m_stopButton->setStyleSheet(QString::fromUtf8("background:transparent; \n"
                                                  "background:#3c3c3c;\n"
                                                  "color: white;\n"
                                                  "border-radius:20px;"));


    timer = new QTimer(this);
    cameraPipe = new videoCapturePip();
    mergedPip = new MergedProcessingPip();

    pip = new pipline();
    qDebug() << "ç»„ä»¶åˆå§‹åŒ–å®Œæˆï¼Œåˆå§‹çŠ¶æ€:" << currentState;

    QString imagePath = "F://opencv_picture//moni.png";
    fieldImage = cv::imread(imagePath.toStdString());

    if (fieldImage.empty()) {
        qWarning() << "æ— æ³•è¯»å–èƒŒæ™¯å›¾åƒ:" << imagePath;
    } else {
        qDebug() << "èƒŒæ™¯å›¾åƒè¯»å–æˆåŠŸ:" << imagePath;
        qDebug() << "åŸå§‹å›¾åƒå°ºå¯¸:" << fieldImage.cols << "x" << fieldImage.rows
                 << "é€šé“æ•°:" << fieldImage.channels();
    }
    if (fieldImage.cols > 1920 || fieldImage.rows > 1080) {
        cv::resize(fieldImage, fieldImage, cv::Size(1920, 1080));
    }


    correctionParams.gainFactor = 1.0;
    correctionParams.maxOffset = 50.0;
    correctionParams.deadZone = 0.5;
    correctionParams.enableCorrection = true;
    smoothingFactor = 0.3;
    nystagmusSimulationActive = false;  // ç¡®ä¿åˆå§‹çŠ¶æ€ä¸ºfalse

    currentOffset = cv::Point2f(0, 0);
    smoothOffset = cv::Point2f(0, 0);

    currentState = STOPPED;
    cameraFlag = false;
    predictionSystem.reset();  // ç¡®ä¿æ¸…æ´çŠ¶æ€

    baseImage = fieldImage.clone();
    image = fieldImage.clone();

    imageSize = cv::Size(IMAGE_WIDTH, IMAGE_HEIGHT);
    imageCenterReference = cv::Point2f(IMAGE_WIDTH / 2.0f, IMAGE_HEIGHT / 2.0f);  // (960, 540)
    currentCorrectionMode = MODE_NORMAL_CORRECTION;

    peakInfo = PeakDetectionInfo();
    qDebug() << "å³°å€¼æ£€æµ‹ç³»ç»Ÿå·²åˆå§‹åŒ–";

    qDebug() << QString("å›ºå®šå‚è€ƒç‚¹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - å›¾åƒä¸­å¿ƒ: (%.1f, %.1f)")
                    .arg(imageCenterReference.x)
                    .arg(imageCenterReference.y);
    qDebug() << QString("å›¾åƒå°ºå¯¸: %1 x %2").arg(IMAGE_WIDTH).arg(IMAGE_HEIGHT);

    qDebug() << "ç»„ä»¶åˆå§‹åŒ–å®Œæˆï¼Œåˆå§‹çŠ¶æ€:" << currentState;
    qDebug() << "çŸ«æ­£ç³»ç»Ÿå‚æ•°å·²åˆå§‹åŒ–ï¼š";
    qDebug() << "  - å¢ç›Šç³»æ•°:" << correctionParams.gainFactor;
    qDebug() << "  - æœ€å¤§åç§»:" << correctionParams.maxOffset;
    qDebug() << "  - æ­»åŒº:" << correctionParams.deadZone;
    qDebug() << "  - å¹³æ»‘ç³»æ•°:" << smoothingFactor;
    qDebug() << "  - åŸºå‡†å›¾åƒå°ºå¯¸:" << baseImage.cols << "x" << baseImage.rows;
}




void eyeTrack::processMergedResult(int frameId, bool success) {
    QElapsedTimer timer;
    timer.start();

    static std::map<int, std::vector<cv::Point2f>> multiFramePredictions;

    static std::map<int, cv::Point2f> alphaBetaNextFramePredictions;
    static std::map<int, cv::Point2f> arxNextFramePredictions;

    static std::map<int, float> alphaBetaPreviousPredictionsX;
    static std::map<int, float> arxPreviousPredictionsX;

    static std::map<int, float> l2l3PreviousPredictionsX;
    static std::map<int, float> l1l2PreviousPredictionsX;
    static std::map<int, float> l1OnlyPreviousPredictionsX;


    static int lastProcessedFrameId = -1;
    static cv::Point2f lastValidGazePoint(960.0f, 540.0f);
    static cv::Point2f lastKnownGoodGazePoint(960.0f, 540.0f);
    static bool hasValidHistory = false;
    static int totalProcessedFrames = 0;

    // çœ¼éœ‡æ£€æµ‹ç›¸å…³é™æ€å˜é‡
    static int nystagmusPeakCount = 0;
    static cv::Point2f lastGazeDirection(0, 0);
    static int directionReversalCount = 0;
    static std::deque<float> velocityHistory;
    static const int VELOCITY_HISTORY_SIZE = 10;


    // é¢„æµ‹æ¥æºè¿½è¸ª
    static std::map<int, int> predictionSourceFrame;
    static std::map<int, cv::Point2f> frameGazePoints;

    totalProcessedFrames++;

    // === ç¬¬1æ­¥ï¼šé˜²é‡å¤å¤„ç† ===
    if (frameId == lastProcessedFrameId) {
        qWarning() << "æ”¶åˆ°é‡å¤çš„å¸§ID:" << frameId;
        return;
    }

    // === ç¬¬2æ­¥ï¼šè·å–å¹¶è¯„ä¼°ä¹‹å‰çš„é¢„æµ‹ ===
    cv::Point2f bestPredictionForThisFrame(960.0f, 540.0f);
    bool hasPrediction = false;
    int predictionSource = -1;

    // æŸ¥æ‰¾æœ€ä½³é¢„æµ‹ï¼ˆæ¥è‡ª1-3å¸§å‰ï¼‰
    for (int lookback = 1; lookback <= 3; lookback++) {
        int sourceFrame = frameId - lookback;
        if (multiFramePredictions.find(sourceFrame) != multiFramePredictions.end()) {
            auto& predictions = multiFramePredictions[sourceFrame];
            if (predictions.size() >= lookback) {
                bestPredictionForThisFrame = predictions[lookback - 1];
                hasPrediction = true;
                predictionSource = sourceFrame;
                predictionSourceFrame[frameId] = sourceFrame;

                if (frameId % 50 == 0) {  // å‡å°‘æ—¥å¿—è¾“å‡ºé¢‘ç‡
                    qDebug() << QString("å¸§%1: ä½¿ç”¨æ¥è‡ªå¸§%2çš„%3æ­¥é¢„æµ‹")
                                    .arg(frameId).arg(sourceFrame).arg(lookback);
                }
                break;
            }
        }
    }
    //éªŒè¯å½“å‰å¸§æ•°æ® ===
    FrameData frameData;
    bool hasValidData = false;
    cv::Point2f currentGazePoint(960.0f, 540.0f);
    ValidationResult validation;
    validation.success = success;

    if (!success) {
        validation.failReason = "MergedProcessingPipå¤„ç†å¤±è´¥";
    } else if (!SharedPipelineData::getFrameData(frameId, frameData)) {
        validation.failReason = "æ— æ³•è·å–å¸§æ•°æ®";
    } else {
        validation.hasFrameData = true;
        validation.imageValid = !frameData.originalImage.empty();
        validation.gazeValid = frameData.gazeValid;
        validation.lightPointsValid = frameData.lightPoints.size() >= 4;
        validation.pupilValid = frameData.pupilCircle.center.x > 0 && frameData.pupilCircle.center.y > 0;

        if (!validation.imageValid) {
            validation.failReason = "åŸå§‹å›¾åƒä¸ºç©º";
        } else if (!validation.gazeValid) {
            validation.failReason = "æ³¨è§†ç‚¹æ— æ•ˆ";
        } else if (!validation.lightPointsValid) {
            validation.failReason = "å…‰æ–‘æ•°é‡ä¸è¶³ï¼š" + std::to_string(frameData.lightPoints.size());
        } else if (!validation.pupilValid) {
            validation.failReason = "ç³å­”ä¸­å¿ƒæ— æ•ˆ";
        } else {
            currentGazePoint = frameData.gazePoint;

            // æ•°æ®åˆç†æ€§æ£€æŸ¥
            if (std::isnan(currentGazePoint.x) || std::isnan(currentGazePoint.y) ||
                std::isinf(currentGazePoint.x) || std::isinf(currentGazePoint.y)) {
                validation.failReason = "æ³¨è§†ç‚¹åŒ…å«NaNæˆ–Infå€¼";
            } else if (std::abs(currentGazePoint.x) > 3000.0f || std::abs(currentGazePoint.y) > 3000.0f) {
                validation.failReason = "æ³¨è§†ç‚¹è¶…å‡ºåˆç†èŒƒå›´";
            } else {
                hasValidData = true;
                lastKnownGoodGazePoint = currentGazePoint;
                hasValidHistory = true;
            }
        }
    }


    if (hasValidData) {

        frameGazePoints[frameId] = currentGazePoint;
        m_actualPredictions[frameId] = bestPredictionForThisFrame;
        m_trueGazePoints[frameId] = currentGazePoint;
        if(peakInfo.compensationActive &&
            peakInfo.compensationFrameCount == 2 &&
            frameId == peakInfo.compensationStartFrame + 1){
            float predictions = bestPredictionForThisFrame.x- currentGazePoint.x;
            if(predictions < 0)
            {
                peakInfo.skipNextCompensation = true;
            }
        }
    }

    // å¤„ç†æ— æ•ˆæ•°æ®çš„æƒ…å†µ
    if (!hasValidData) {
        if (frameId % 30 == 0) {  // å‡å°‘æ— æ•ˆæ•°æ®çš„æ—¥å¿—è¾“å‡º
            qWarning() << "å¸§" << frameId << "æ•°æ®æ— æ•ˆï¼š" << QString::fromStdString(validation.failReason);
        }

        // å°è¯•ä½¿ç”¨å†å²é¢„æµ‹
        if (hasValidHistory) {
            std::vector<cv::Point2f> fallbackPredictions(3, lastKnownGoodGazePoint);
            multiFramePredictions[frameId] = fallbackPredictions;
            m_nextFramePredictions[frameId] = fallbackPredictions[0];
        }

        lastProcessedFrameId = frameId;
        return;
    }

    //ç”Ÿæˆæ–°çš„é¢„æµ‹
    QElapsedTimer predictTimer;
    predictTimer.start();

    double processingTimeMs = 0;
    std::string diagnosticInfo;

    // åŸæœ‰çš„BalancedLowLatencyPredictoré¢„æµ‹
    cv::Point2f currentPrediction = predictionSystem.processFrame(
        currentGazePoint, frameId, processingTimeMs, diagnosticInfo);

    // double alphaBetaProcessingTime = 0;
    // std::string alphaBetaDiagnostic;
    // cv::Point2f alphaBetaNextPrediction = alphaBetaPredictor.processFrame(
    //     currentGazePoint, frameId, alphaBetaProcessingTime, alphaBetaDiagnostic);

    // // ARXPredictoré¢„æµ‹ï¼ˆè¿”å›å¯¹ä¸‹ä¸€å¸§çš„é¢„æµ‹ï¼‰
    // double arxProcessingTime = 0;
    // std::string arxDiagnostic;
    // cv::Point2f arxNextPrediction = arxPredictor.processFrame(
    //     currentGazePoint, frameId, arxProcessingTime, arxDiagnostic);


    // double kalmanProcessingTime = 0;
    // std::string kalmanDiagnostic;
    // cv::Point2f kalmanNextPrediction = kalmanPredictor.processFrame(
    //     currentGazePoint, frameId, kalmanProcessingTime, kalmanDiagnostic);

    // double l2l3ProcessingTime = 0;
    // std::string l2l3Diagnostic;
    // cv::Point2f l2l3NextPrediction = l2l3Predictor.processFrame(
    //     currentGazePoint, frameId, l2l3ProcessingTime, l2l3Diagnostic);

    // // L1+L2é¢„æµ‹å™¨ï¼ˆæ— è¶‹åŠ¿å¢å¼ºï¼‰
    // double l1l2ProcessingTime = 0;
    // std::string l1l2Diagnostic;
    // cv::Point2f l1l2NextPrediction = l1l2Predictor.processFrame(
    //     currentGazePoint, frameId, l1l2ProcessingTime, l1l2Diagnostic);

    // // ä»…L1é¢„æµ‹å™¨ï¼ˆè‡ªé€‚åº”å‰ç»ï¼‰
    // double l1OnlyProcessingTime = 0;
    // std::string l1OnlyDiagnostic;
    // cv::Point2f l1OnlyNextPrediction = l1OnlyPredictor.processFrame(
    //     currentGazePoint, frameId, l1OnlyProcessingTime, l1OnlyDiagnostic);



    // // æ·»åŠ é™æ€å˜é‡å­˜å‚¨å¡å°”æ›¼çš„é¢„æµ‹ï¼ˆåœ¨å…¶ä»–é™æ€å˜é‡é™„è¿‘ï¼‰
    // static std::map<int, float> kalmanPreviousPredictionsX;

    // // å¤„ç†å¡å°”æ›¼é¢„æµ‹çš„æ—¶åºï¼ˆåœ¨å¤„ç†å…¶ä»–é¢„æµ‹å™¨åé¢ï¼‰
    // if (kalmanPreviousPredictionsX.find(frameId) != kalmanPreviousPredictionsX.end()) {
    //     m_kalmanPredictionsX[frameId] = kalmanPreviousPredictionsX[frameId];
    // } else {
    //     // ä½¿ç”¨å·²ä¿å­˜çš„ä¸Šä¸€å¸§æ•°æ®ï¼Œè€Œä¸æ˜¯ä»æœªæ¥é¢„æµ‹mapä¸­å–
    //     if (frameId > 0 && m_kalmanPredictionsX.find(frameId-1) != m_kalmanPredictionsX.end()) {
    //         m_kalmanPredictionsX[frameId] = m_kalmanPredictionsX[frameId-1];
    //     } else {
    //         m_kalmanPredictionsX[frameId] = 960.0f; // é»˜è®¤å€¼ï¼ˆå±å¹•ä¸­å¿ƒXåæ ‡ï¼‰
    //     }
    // }

    // // ä¿å­˜å½“å‰å¸§çš„çœŸå®å€¼
    // m_actualGazeX[frameId] = currentGazePoint.x;

    // // AlphaBetaé¢„æµ‹
    // if (alphaBetaPreviousPredictionsX.find(frameId) != alphaBetaPreviousPredictionsX.end()) {
    //     m_alphaBetaPredictionsX[frameId] = alphaBetaPreviousPredictionsX[frameId];
    // } else {
    //     if (frameId > 0 && m_alphaBetaPredictionsX.find(frameId-1) != m_alphaBetaPredictionsX.end()) {
    //         m_alphaBetaPredictionsX[frameId] = m_alphaBetaPredictionsX[frameId-1];
    //     } else {
    //         m_alphaBetaPredictionsX[frameId] = 960.0f;
    //     }
    // }

    // // ARXé¢„æµ‹
    // if (arxPreviousPredictionsX.find(frameId) != arxPreviousPredictionsX.end()) {
    //     m_arxPredictionsX[frameId] = arxPreviousPredictionsX[frameId];
    // } else {
    //     if (frameId > 0 && m_arxPredictionsX.find(frameId-1) != m_arxPredictionsX.end()) {
    //         m_arxPredictionsX[frameId] = m_arxPredictionsX[frameId-1];
    //     } else {
    //         m_arxPredictionsX[frameId] = 960.0f;
    //     }
    // }

    // // L2+L3é¢„æµ‹
    // if (l2l3PreviousPredictionsX.find(frameId) != l2l3PreviousPredictionsX.end()) {
    //     m_l2l3PredictionsX[frameId] = l2l3PreviousPredictionsX[frameId];
    // } else {
    //     if (frameId > 0 && m_l2l3PredictionsX.find(frameId-1) != m_l2l3PredictionsX.end()) {
    //         m_l2l3PredictionsX[frameId] = m_l2l3PredictionsX[frameId-1];
    //     } else {
    //         m_l2l3PredictionsX[frameId] = 960.0f;
    //     }
    // }

    // // L1+L2é¢„æµ‹
    // if (l1l2PreviousPredictionsX.find(frameId) != l1l2PreviousPredictionsX.end()) {
    //     m_l1l2PredictionsX[frameId] = l1l2PreviousPredictionsX[frameId];
    // } else {
    //     if (frameId > 0 && m_l1l2PredictionsX.find(frameId-1) != m_l1l2PredictionsX.end()) {
    //         m_l1l2PredictionsX[frameId] = m_l1l2PredictionsX[frameId-1];
    //     } else {
    //         m_l1l2PredictionsX[frameId] = 960.0f;
    //     }
    // }

    // // ä»…L1é¢„æµ‹
    // if (l1OnlyPreviousPredictionsX.find(frameId) != l1OnlyPreviousPredictionsX.end()) {
    //     m_l1OnlyPredictionsX[frameId] = l1OnlyPreviousPredictionsX[frameId];
    // } else {
    //     if (frameId > 0 && m_l1OnlyPredictionsX.find(frameId-1) != m_l1OnlyPredictionsX.end()) {
    //         m_l1OnlyPredictionsX[frameId] = m_l1OnlyPredictionsX[frameId-1];
    //     } else {
    //         m_l1OnlyPredictionsX[frameId] = 960.0f;
    //     }
    // }


    // // ä¿å­˜å¯¹ä¸‹ä¸€å¸§çš„é¢„æµ‹
    // kalmanPreviousPredictionsX[frameId + 1] = kalmanNextPrediction.x;

    // alphaBetaPreviousPredictionsX[frameId + 1] = alphaBetaNextPrediction.x;
    // arxPreviousPredictionsX[frameId + 1] = arxNextPrediction.x;

    // l2l3PreviousPredictionsX[frameId + 1] = l2l3NextPrediction.x;
    // l1l2PreviousPredictionsX[frameId + 1] = l1l2NextPrediction.x;
    // l1OnlyPreviousPredictionsX[frameId + 1] = l1OnlyNextPrediction.x;

    // æ¸…ç†æ—§çš„é¢„æµ‹æ•°æ®ï¼ˆé¿å…å†…å­˜æ³„æ¼ï¼‰
    // if (alphaBetaPreviousPredictionsX.size() > 500) {
    //     for (auto it = alphaBetaPreviousPredictionsX.begin(); it != alphaBetaPreviousPredictionsX.end(); ) {
    //         if (it->first < frameId - 400) {
    //             it = alphaBetaPreviousPredictionsX.erase(it);
    //         } else {
    //             ++it;
    //         }
    //     }
    // }
    // if (arxPreviousPredictionsX.size() > 500) {
    //     for (auto it = arxPreviousPredictionsX.begin(); it != arxPreviousPredictionsX.end(); ) {
    //         if (it->first < frameId - 400) {
    //             it = arxPreviousPredictionsX.erase(it);
    //         } else {
    //             ++it;
    //         }
    //     }
    // }


    // æ¸…ç†æ—§æ•°æ®
    // if (kalmanPreviousPredictionsX.size() > 500) {
    //     for (auto it = kalmanPreviousPredictionsX.begin(); it != kalmanPreviousPredictionsX.end(); ) {
    //         if (it->first < frameId - 400) {
    //             it = kalmanPreviousPredictionsX.erase(it);
    //         } else {
    //             ++it;
    //         }
    //     }
    // }

    // if (frameId % 100 == 0) {
    //     qDebug() << QString("=== é¢„æµ‹å™¨Xè½´æ¯”è¾ƒ [å¸§%1] ===").arg(frameId);
    //     qDebug() << QString("çœŸå®å€¼X: %.2f").arg(currentGazePoint.x);

    //     // AlphaBetaé¢„æµ‹
    //     if (m_alphaBetaPredictionsX.find(frameId) != m_alphaBetaPredictionsX.end() &&
    //         m_alphaBetaPredictionsX[frameId] != currentGazePoint.x) {
    //         qDebug() << QString("AlphaBetaé¢„æµ‹X: %.2f (è¯¯å·®: %.2f)")
    //                         .arg(m_alphaBetaPredictionsX[frameId])
    //                         .arg(std::abs(m_alphaBetaPredictionsX[frameId] - currentGazePoint.x));
    //     }

    //     // ARXé¢„æµ‹
    //     if (m_arxPredictionsX.find(frameId) != m_arxPredictionsX.end() &&
    //         m_arxPredictionsX[frameId] != currentGazePoint.x) {
    //         qDebug() << QString("ARXé¢„æµ‹X: %.2f (è¯¯å·®: %.2f)")
    //                         .arg(m_arxPredictionsX[frameId])
    //                         .arg(std::abs(m_arxPredictionsX[frameId] - currentGazePoint.x));
    //     }

    //     // Kalmané¢„æµ‹
    //     if (m_kalmanPredictionsX.find(frameId) != m_kalmanPredictionsX.end() &&
    //         m_kalmanPredictionsX[frameId] != currentGazePoint.x) {
    //         qDebug() << QString("Kalmané¢„æµ‹X: %.2f (è¯¯å·®: %.2f)")
    //                         .arg(m_kalmanPredictionsX[frameId])
    //                         .arg(std::abs(m_kalmanPredictionsX[frameId] - currentGazePoint.x));
    //     }

    //     // Balancedé¢„æµ‹
    //     qDebug() << QString("Balancedé¢„æµ‹X: %.2f (è¯¯å·®: %.2f)")
    //                     .arg(currentPrediction.x)
    //                     .arg(std::abs(currentPrediction.x - currentGazePoint.x));

    //     // æ˜¾ç¤ºå¯¹ä¸‹ä¸€å¸§çš„é¢„æµ‹
    //     qDebug() << QString("ä¸‹ä¸€å¸§é¢„æµ‹ - AlphaBeta: %.2f, ARX: %.2f, Kalman: %.2f")
    //                     .arg(alphaBetaNextPrediction.x)
    //                     .arg(arxNextPrediction.x)
    //                     .arg(kalmanNextPrediction.x);
    // }


    // ç”Ÿæˆå¤šæ­¥é¢„æµ‹ ===
    std::vector<cv::Point2f> futurePredictions;

    try {
        // ä½¿ç”¨ BalancedLowLatencyPredictor çš„å¤šæ­¥é¢„æµ‹èƒ½åŠ›
        futurePredictions = predictionSystem.getMultiStepPredictions(frameId);

        // ç¡®ä¿é¢„æµ‹å€¼æœ‰æ•ˆ
        for (size_t i = 0; i < futurePredictions.size(); i++) {
            if (std::isnan(futurePredictions[i].x) || std::isnan(futurePredictions[i].y) ||
                futurePredictions[i].x < 0 || futurePredictions[i].x > 1920 ||
                futurePredictions[i].y < 0 || futurePredictions[i].y > 1080) {

                qWarning() << QString("å¸§%1: ç¬¬%2æ­¥é¢„æµ‹æ— æ•ˆï¼Œä½¿ç”¨å½“å‰é¢„æµ‹å€¼")
                                  .arg(frameId).arg(i + 1);
                futurePredictions[i] = currentPrediction;
            }
        }

    } catch (const std::exception& e) {
        qWarning() << "å¤šæ­¥é¢„æµ‹ç”Ÿæˆå¤±è´¥:" << e.what();
        // å›é€€æ–¹æ¡ˆï¼šåŸºäºå½“å‰é¢„æµ‹å’Œé€Ÿåº¦ç”Ÿæˆ
        futurePredictions.clear();

        cv::Point2f velocity(0, 0);
        if (hasValidHistory && !m_trueGazePoints.empty()) {
            auto lastIt = m_trueGazePoints.find(frameId - 1);
            if (lastIt != m_trueGazePoints.end()) {
                velocity = currentGazePoint - lastIt->second;
            }
        }

        // ç”Ÿæˆç®€å•çš„çº¿æ€§é¢„æµ‹
        for (int step = 1; step <= 3; step++) {
            cv::Point2f futurePoint = currentPrediction + velocity * (step * 0.5f);

            // è¾¹ç•Œæ£€æŸ¥
            futurePoint.x = std::max(0.0f, std::min(1920.0f, futurePoint.x));
            futurePoint.y = std::max(0.0f, std::min(1080.0f, futurePoint.y));

            futurePredictions.push_back(futurePoint);
        }
    }

    // ç¡®ä¿æœ‰3ä¸ªé¢„æµ‹å€¼
    while (futurePredictions.size() < 3) {
        futurePredictions.push_back(currentPrediction);
    }

    //  çœ¼éœ‡ç‰¹å¾åˆ†æ ===
    cv::Point2f currentDirection = currentGazePoint - lastValidGazePoint;
    float currentVelocity = cv::norm(currentDirection);

    // æ›´æ–°é€Ÿåº¦å†å²
    velocityHistory.push_back(currentVelocity);
    if (velocityHistory.size() > VELOCITY_HISTORY_SIZE) {
        velocityHistory.pop_front();
    }

    // === å³°å€¼æ£€æµ‹é€»è¾‘ ===
    bool peakDetected = false;

    if (hasValidData) {
        peakDetected = detectSimplePeak(currentGazePoint, frameId);

        if (peakDetected) {
            // è®¾ç½®åŸºå‡†è¯¯å·®ç”¨äºè¡¥å¿
            cv::Point2f detectionFrameError;
            if (m_actualPredictions.find(frameId) != m_actualPredictions.end()) {
                detectionFrameError = m_actualPredictions[frameId] - currentGazePoint;
            } else {
                detectionFrameError = futurePredictions[0] - currentGazePoint;
            }
            peakInfo.baseCompensationError = detectionFrameError;
        }
    }


    lastGazeDirection = currentDirection;
    cv::Point2f detectionFrameError;

    if (peakDetected) {
        peakInfo.lastPeakFrame = frameId - 1;
        peakInfo.compensationStartFrame = frameId;
        peakInfo.compensationActive = true;


        // ä¼˜å…ˆä½¿ç”¨æ¥è‡ªä¹‹å‰å¸§çš„é¢„æµ‹ï¼ˆè¿™æ˜¯çœŸæ­£çš„é¢„æµ‹å€¼ï¼‰
        if (m_actualPredictions.find(frameId) != m_actualPredictions.end()) {
            cv::Point2f actualPrediction = m_actualPredictions[frameId];
            detectionFrameError = actualPrediction - currentGazePoint;

            qDebug() << QString("ä½¿ç”¨å®é™…é¢„æµ‹ä½œä¸ºåŸºå‡†: é¢„æµ‹=(%1,%2), çœŸå®=(%3,%4)")
                            .arg(actualPrediction.x).arg(actualPrediction.y)
                            .arg(currentGazePoint.x).arg(currentGazePoint.y);
        } else {
            // å›é€€ï¼šä½¿ç”¨å½“å‰ç”Ÿæˆçš„é¢„æµ‹
            detectionFrameError = futurePredictions[0] - currentGazePoint;

            qDebug() << QString(" ä½¿ç”¨å½“å‰é¢„æµ‹ä½œä¸ºåŸºå‡†: é¢„æµ‹=(%1,%2), çœŸå®=(%3,%4)")
                            .arg(futurePredictions[0].x).arg(futurePredictions[0].y)
                            .arg(currentGazePoint.x).arg(currentGazePoint.y);
        }

        peakInfo.baseCompensationError = detectionFrameError;

        // ä¿å­˜å…¶ä»–å³°å€¼ä¿¡æ¯
        peakInfo.lastPeakVelocity = (velocityHistory.size() >= 2) ?
                                        velocityHistory[velocityHistory.size() - 2] : currentVelocity;
        peakInfo.lastPeakDirection = lastGazeDirection;

        qDebug() << QString("å³°å€¼æ£€æµ‹[å¸§%1]: å³°å€¼å¸§=%2, åŸºå‡†è¯¯å·®=(%3,%4), è¯¯å·®å¹…åº¦=%5px, è¡¥å¿%6å¸§")
                        .arg(frameId)
                        .arg(frameId - 1)
                        .arg(detectionFrameError.x).arg(detectionFrameError.y)
                        .arg(cv::norm(detectionFrameError))
                        .arg(peakInfo.compensationFrameCount);
    }

    // ä¿®æ”¹ï¼šåŠ¨æ€è¡¥å¿å¸§æ•°åº”ç”¨
    if (peakInfo.compensationActive) {
        int framesSincePeak = frameId - peakInfo.compensationStartFrame;
        int maxCompensationFrames = peakInfo.compensationFrameCount - 1;  // è½¬æ¢ä¸º0-basedç´¢å¼•

        if (framesSincePeak >= 0 && framesSincePeak <= maxCompensationFrames) {
            float compensationFactor = 0;
            bool shouldApplyCompensation = true;

            if (peakInfo.compensationFrameCount == 2) {
                // 2å¸§è¡¥å¿ç­–ç•¥ (X > 600)
                if(detectionFrameError.x < 100)
                    detectionFrameError.x = 100;
                if (framesSincePeak == 0) {
                    compensationFactor = 0.7f;      // ç¬¬1å¸§ï¼š60%å‡å°
                } else if (framesSincePeak == 1) {
                    if(peakInfo.skipNextCompensation){
                        shouldApplyCompensation = false;
                        peakInfo.skipNextCompensation = false;
                        qDebug()<<"ç¬¬ä¸€å¸§è¡¥å¿è¿‡äº†ï¼Œåœæ­¢è¡¥å¿ å¸§ï¼š"<<frameId;
                    }
                    else
                        compensationFactor = 0.4f;      // ç¬¬2å¸§ï¼š40%å‡å°

                }
            } else {
                // 1å¸§è¡¥å¿ç­–ç•¥ (X <= 600)
                if (framesSincePeak == 0) {
                    compensationFactor = 0.55f;      // ç¬¬1å¸§ï¼š70%å‡å°
                }
            }

            // ä½¿ç”¨æ£€æµ‹å¸§çš„è¯¯å·®ä½œä¸ºåŸºå‡†ï¼Œè€Œä¸æ˜¯å½“å‰å¸§è¯¯å·®
            cv::Point2f baseError = peakInfo.baseCompensationError;  // æ£€æµ‹å¸§çš„è¯¯å·®
            cv::Point2f reduction = baseError * compensationFactor;  // æŒ‰æ¯”ä¾‹å‡å°

            // åº”ç”¨å‡å°è¡¥å¿
            cv::Point2f originalPrediction = futurePredictions[0];
            futurePredictions[0] -= reduction;  // å‡å»è¯¯å·®çš„ä¸€éƒ¨åˆ†

            // è¾¹ç•Œæ£€æŸ¥
            futurePredictions[0].x = std::max(0.0f, std::min(1920.0f, futurePredictions[0].x));
            futurePredictions[0].y = std::max(0.0f, std::min(1080.0f, futurePredictions[0].y));

            qDebug() << QString("åŠ¨æ€è¡¥å¿[å¸§%1]: ç¬¬%2/%3å¸§, ç³»æ•°=%4f, åŸºå‡†è¯¯å·®=(%5,%6), å‡å°%7fpx")
                            .arg(frameId)
                            .arg(framesSincePeak + 1)
                            .arg(peakInfo.compensationFrameCount)
                            .arg(compensationFactor)
                            .arg(baseError.x).arg(baseError.y)
                            .arg(cv::norm(reduction));

            qDebug() << QString("   é¢„æµ‹å˜åŒ–: (%1f,%2f) â†’ (%3f,%4f)")
                            .arg(originalPrediction.x).arg(originalPrediction.y)
                            .arg(futurePredictions[0].x).arg(futurePredictions[0].y);

            QString compensationMsg = QString("ğŸ”§ åŠ¨æ€è¡¥å¿[å¸§%1]: ç¬¬%2/%3å¸§, ç³»æ•°=%4f, å‡å°%5fpx")
                                          .arg(frameId)
                                          .arg(framesSincePeak + 1)
                                          .arg(peakInfo.compensationFrameCount)
                                          .arg(compensationFactor)
                                          .arg(cv::norm(reduction));

        } else if (framesSincePeak > maxCompensationFrames) {
            // è¡¥å¿ç»“æŸ
            peakInfo.compensationActive = false;
            qDebug() << QString("åŠ¨æ€è¡¥å¿ç»“æŸ[å¸§%1]: å®Œæˆ%2å¸§è¡¥å¿")
                            .arg(frameId).arg(peakInfo.compensationFrameCount);
        }
    }

    // å­˜å‚¨é¢„æµ‹ç»“æœ
    multiFramePredictions[frameId] = futurePredictions;
    m_nextFramePredictions[frameId] = futurePredictions[0];

    // å­˜å‚¨å…¶ä»–æ•°æ®
    //æ³¨è§†ç‚¹ç›¸å…³ä¿¡æ¯
    std::vector<cv::Point2f> currentLightPoints(4);
    for(int i = 0; i < 4 && i < frameData.lightPoints.size(); i++){
        currentLightPoints[i] = frameData.lightPoints[i].center;
    }

    lightTotal[frameId] = currentLightPoints;
    pupilTotal[frameId] = frameData.pupilCircle.center;
    angelTotal[frameId] = frameData.pupilCircle.angle;
    areaTotal[frameId] = frameData.pupilCircle.area;
    eccentricityTotal[frameId] = frameData.pupilCircle.eccentricity;
    circularityTotal[frameId] = frameData.pupilCircle.circularity;

    // å¤„ç†æ—¶é—´
    double preTime = predictTimer.nsecsElapsed() / 1e6;
    videoCaptureTime[frameId] = frameData.capTime;
    pupilTime[frameId] = frameData.pupilTime;
    roiTime[frameId] = frameData.rolTime;
    spotTime[frameId] = frameData.spotTime;
    predictTime[frameId] = preTime;

    lastValidGazePoint = currentGazePoint;

    // === ç¬¬8æ­¥ï¼šæ¸…ç†æ—§æ•°æ® ===
    if (multiFramePredictions.size() > 300) {
        auto oldest = multiFramePredictions.begin();
        multiFramePredictions.erase(oldest);
    }

    if (frameGazePoints.size() > 500) {
        for (auto it = frameGazePoints.begin(); it != frameGazePoints.end(); ) {
            if (it->first < frameId - 400) {
                it = frameGazePoints.erase(it);
            } else {
                ++it;
            }
        }
    }

    if (predictionSourceFrame.size() > 500) {
        for (auto it = predictionSourceFrame.begin(); it != predictionSourceFrame.end(); ) {
            if (it->first < frameId - 400) {
                it = predictionSourceFrame.erase(it);
            } else {
                ++it;
            }
        }
    }

    lastProcessedFrameId = frameId;

    // === ç¬¬9æ­¥ï¼šå›¾åƒæ˜¾ç¤ºå’ŒUIæ›´æ–° ===
    if (!nystagmusSimulationActive) {
        baseImage = fieldImage.clone();
        image = fieldImage.clone();
    }

    QElapsedTimer DrawTimer;
    DrawTimer.start();

    // ç»˜åˆ¶å’Œæ˜¾ç¤º
    drawParallelMarkersAndDisplay(frameData.originalImage, &frameData, frameId);
    double Time = DrawTimer.nsecsElapsed() / 1e6;
    DrawTime[frameId] = Time;


    // å‘é€å›¾è¡¨æ›´æ–°ä¿¡å·
    if (frameData.gazeValid) {
        emit chartSignals();
    }

    // === ç¬¬10æ­¥ï¼šæ€§èƒ½ç›‘æ§å’ŒæŠ¥å‘Š ===
    // æ›´æ–°æ€§èƒ½ç»Ÿè®¡
    if (hasPrediction) {
        double error = cv::norm(currentGazePoint - bestPredictionForThisFrame);
        if (error < 1000) {  // è¿‡æ»¤å¼‚å¸¸å€¼
            performanceStats.totalFrames++;
            performanceStats.horizontalErrorSum += std::abs(currentGazePoint.x - bestPredictionForThisFrame.x);
            performanceStats.verticalErrorSum += std::abs(currentGazePoint.y - bestPredictionForThisFrame.y);

            if (error < 5.0) {
                performanceStats.highPrecisionFrames++;
            }

            performanceStats.recentErrors.push_back(error);
            if (performanceStats.recentErrors.size() > performanceStats.ERROR_WINDOW) {
                performanceStats.recentErrors.pop_front();
            }
        }
    }

    // å®šæœŸè¯¦ç»†æŠ¥å‘Šï¼ˆæ¯100å¸§ï¼‰
    if (frameId % 100 == 0 && performanceStats.totalFrames > 0) {
        double avgHorizontalError = performanceStats.horizontalErrorSum / performanceStats.totalFrames;
        double avgVerticalError = performanceStats.verticalErrorSum / performanceStats.totalFrames;
        double precision = (double)performanceStats.highPrecisionFrames / performanceStats.totalFrames * 100;

        QString predictionStatus = QString("ç®€åŒ–çœ¼éœ‡é¢„æµ‹[%1å¸§]: æ°´å¹³è¯¯å·®=%2px, å‚ç›´è¯¯å·®=%3px, é«˜ç²¾åº¦ç‡=%4%")
                                       .arg(performanceStats.totalFrames)
                                       .arg(avgHorizontalError, 0, 'f', 2)
                                       .arg(avgVerticalError, 0, 'f', 2)
                                       .arg(precision, 0, 'f', 1);

        // é¢„æµ‹å™¨è´¨é‡è¯„åˆ†
        double quality = predictionSystem.getPredictionQuality();
        QString qualityStatus = QString("é¢„æµ‹è´¨é‡è¯„åˆ†: %1/1.0 | å¤„ç†æ—¶é—´: %2ms")
                                    .arg(quality, 0, 'f', 2)
                                    .arg(processingTimeMs, 0, 'f', 2);

        // çœ¼éœ‡ç‰¹å¾ç»Ÿè®¡
        if (directionReversalCount > 0) {
            double timeInSeconds = frameId / 60.0;
            double nystagmusFreq = directionReversalCount / (2.0 * timeInSeconds);
            QString nystagmusStatus = QString("çœ¼éœ‡ç‰¹å¾: æ–¹å‘åè½¬%1æ¬¡, é¢‘ç‡çº¦%2Hz")
                                          .arg(directionReversalCount)
                                          .arg(nystagmusFreq, 0, 'f', 2);
        }

        // é¢„æµ‹å™¨è¯Šæ–­ä¿¡æ¯
        QString diagnostics = QString::fromStdString(predictionSystem.getDiagnosticInfo());
        if (frameId % 200 == 0) {  // æ¯200å¸§è¾“å‡ºä¸€æ¬¡è¯¦ç»†è¯Šæ–­
            qDebug() << "ç®€åŒ–é¢„æµ‹ç³»ç»Ÿè¯Šæ–­:\n" << diagnostics;
        }
    }

    // å®æ—¶æ€§èƒ½ç›‘æ§ï¼ˆæ¯30å¸§ï¼‰
    if (frameId % 30 == 0) {
        double recentAvgError = 0;
        if (!performanceStats.recentErrors.empty()) {
            recentAvgError = std::accumulate(performanceStats.recentErrors.begin(),
                                             performanceStats.recentErrors.end(), 0.0) /
                             performanceStats.recentErrors.size();
        }

        // çœ¼éœ‡çŠ¶æ€æŒ‡ç¤º
        QString nystagmusIndicator = "";
        if (!velocityHistory.empty()) {
            float avgVelocity = std::accumulate(velocityHistory.begin(), velocityHistory.end(), 0.0f)
            / velocityHistory.size();
            if (avgVelocity > 100) {
                nystagmusIndicator = " | çœ¼éœ‡:æ´»è·ƒ";
            } else if (avgVelocity > 50) {
                nystagmusIndicator = " | çœ¼éœ‡:ä¸­ç­‰";
            } else {
                nystagmusIndicator = " | çœ¼éœ‡:å¹³é™";
            }
        }

        // é¢„æµ‹å™¨çŠ¶æ€ä¿¡æ¯
        double predictionQuality = predictionSystem.getPredictionQuality();
        QString qualityIndicator = "";
        if (predictionQuality > 0.9) {
            qualityIndicator = " | è´¨é‡:ä¼˜ç§€";
        } else if (predictionQuality > 0.8) {
            qualityIndicator = " | è´¨é‡:è‰¯å¥½";
        } else if (predictionQuality > 0.7) {
            qualityIndicator = " | è´¨é‡:å¯æ¥å—";
        } else {
            qualityIndicator = " | è´¨é‡:éœ€æ”¹è¿›";
        }

        // æ›´æ–°æ€§èƒ½æ ‡ç­¾
        QString perfText = QString("ç®€åŒ–é¢„æµ‹ç³»ç»Ÿ | å¸§:%1 | å®æ—¶è¯¯å·®:%2px%3%4")
                               .arg(frameId)
                               .arg(recentAvgError, 0, 'f', 1)
                               .arg(nystagmusIndicator)
                               .arg(qualityIndicator);

        if (performanceLabel) {
            performanceLabel->setText(perfText);

            // æ ¹æ®è¯¯å·®å’Œè´¨é‡è®¾ç½®é¢œè‰²
            if (recentAvgError < 10 && predictionQuality > 0.9) {
                performanceLabel->setStyleSheet("color: green; font-weight: bold; background-color: rgba(0,255,0,0.1); padding: 5px;");
            } else if (recentAvgError < 20 && predictionQuality > 0.8) {
                performanceLabel->setStyleSheet("color: orange; font-weight: bold; background-color: rgba(255,165,0,0.1); padding: 5px;");
            } else {
                performanceLabel->setStyleSheet("color: red; font-weight: bold; background-color: rgba(255,0,0,0.1); padding: 5px;");
            }
        }
    }

    // è¯¦ç»†è°ƒè¯•è¾“å‡ºï¼ˆæ¯200å¸§ï¼‰
    if (frameId % 200 == 0) {
        cv::Point2f predictionError = currentPrediction - currentGazePoint;
        qDebug() << "\n=== ç®€åŒ–é¢„æµ‹ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š ===";
        qDebug() << QString("å¸§ %1: å¤„ç†æ—¶é—´ %2ms").arg(frameId).arg(timer.elapsed());
        qDebug() << QString("å½“å‰æ³¨è§†ç‚¹: (%.2f, %.2f)").arg(currentGazePoint.x).arg(currentGazePoint.y);
        qDebug() << QString("é¢„æµ‹ç»“æœ: (%.2f, %.2f)").arg(currentPrediction.x).arg(currentPrediction.y);
        qDebug() << QString("é¢„æµ‹è¯¯å·®: (%.2f, %.2f) | å¹…åº¦: %.2f px")
                        .arg(predictionError.x).arg(predictionError.y).arg(cv::norm(predictionError));
        qDebug() << QString("å¤šæ­¥é¢„æµ‹: %1æ­¥").arg(futurePredictions.size());
        qDebug() << QString("çœ¼éœ‡ç»Ÿè®¡: %1æ¬¡åè½¬, å¹³å‡é€Ÿåº¦%.1f px/å¸§")
                        .arg(directionReversalCount)
                        .arg(velocityHistory.empty() ? 0 :
                                 std::accumulate(velocityHistory.begin(), velocityHistory.end(), 0.0f) / velocityHistory.size());
        qDebug() << QString("é¢„æµ‹å™¨è´¨é‡: %.1f%%, è¯Šæ–­: %2")
                        .arg(predictionSystem.getPredictionQuality() * 100)
                        .arg(QString::fromStdString(diagnosticInfo).left(100));  // åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
        qDebug() << "=====================================\n";
    }


}

void eyeTrack::drawParallelMarkersAndDisplay(cv::Mat& rgbImage, FrameData *frameData, int frameId)
{
    cv::Mat originalCopy = rgbImage.clone();

    // ç»˜åˆ¶å…‰æ–‘ï¼ˆä¸åŸç‰ˆä¿æŒä¸€è‡´ï¼‰
    for(size_t i = 0; i < frameData->lightPoints.size(); ++i) {
        const auto& spot = frameData->lightPoints[i];
        // ç»˜åˆ¶å…‰æ–‘åœ†ç‚¹
        cv::circle(rgbImage, spot.center, 3, cv::Scalar(0, 0, 255), -1);
        // æ·»åŠ å…‰æ–‘ç¼–å·
        std::string spotText = std::to_string(i + 1);
        cv::Point textPos = spot.center;
        textPos.x += 5;
        textPos.y -= 5;
        cv::putText(rgbImage, spotText, textPos, cv::FONT_HERSHEY_SIMPLEX,
                    0.5, cv::Scalar(0, 0, 255), 1);
    }

    visualizePupilDetection(rgbImage, frameData->pupilCircle);

    // åœ¨ç³å­”ä¸­å¿ƒæ·»åŠ æ–‡å­—æ ‡è¯†
    std::string pupilText = "P";
    cv::Point textPos = frameData->pupilCircle.center;
    textPos.y -= frameData->pupilCircle.center.y + 10;
    cv::putText(rgbImage, pupilText, textPos, cv::FONT_HERSHEY_SIMPLEX,
                0.6, cv::Scalar(0, 255, 0), 2);

    // æ·»åŠ æ€»ä½“ä¿¡æ¯æ˜¾ç¤º
    std::string infoText = "Parallel Frame: " + std::to_string(frameId) +
                           " | Spots: " + std::to_string(frameData->lightPoints.size());
    cv::putText(rgbImage, infoText, cv::Point(10, 30), cv::FONT_HERSHEY_SIMPLEX,
                0.6, cv::Scalar(255, 255, 255), 2);


    // è½¬æ¢å¹¶æ˜¾ç¤º
    QImage qimg = QImage(rgbImage.data, rgbImage.cols, rgbImage.rows, rgbImage.step, QImage::Format_RGB888);
    QPixmap pixmap = QPixmap::fromImage(qimg);

    // ğŸ”§ è·å–labelå°ºå¯¸ï¼Œå‡å»ä¸€äº›è¾¹è·
    QSize labelSize = ui->displayLabel->size();
    QSize adjustedSize(labelSize.width() - 20, labelSize.height() - 20);  // å‡å»20åƒç´ ä½œä¸ºè¾¹è·

    // ğŸ”§ æŒ‰è°ƒæ•´åçš„å°ºå¯¸ç¼©æ”¾
    QPixmap centeredPixmap = pixmap.scaled(adjustedSize, Qt::KeepAspectRatio, Qt::SmoothTransformation);

    ui->displayLabel->setPixmap(centeredPixmap);

    // ä¿å­˜å›¾åƒ
    if(dataFlag) {
        imageSave.addDisplayImageToBuffer(rgbImage,frameId);
        imageSave.addOriginalImageToBuffer(originalCopy,frameId);
    }
}

void eyeTrack::displayImage(const cv::Mat& image) {
    if (image.empty()) {
        qDebug() << "å°è¯•æ˜¾ç¤ºç©ºå›¾åƒ";
        return;
    }

    QImage qimg;

    // æ ¹æ®å›¾åƒé€šé“æ•°è¿›è¡Œè½¬æ¢
    if (image.channels() == 1) {
        qimg = QImage(image.data, image.cols, image.rows, image.step, QImage::Format_Grayscale8);
    } else if (image.channels() == 3) {
        qimg = QImage(image.data, image.cols, image.rows, image.step, QImage::Format_RGB888);
    } else {
        qDebug() << "ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼ï¼Œé€šé“æ•°ï¼š" << image.channels();
        return;
    }

    if (qimg.isNull()) {
        qDebug() << "QImage è½¬æ¢å¤±è´¥";
        return;
    }

    // === å…³é”®ä¿®å¤ï¼šç¡®ä¿åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°UI ===
    QMetaObject::invokeMethod(this, [this, qimg]() {
        ui->displayLabel->setPixmap(QPixmap::fromImage(qimg).scaled(
            ui->displayLabel->size(), Qt::KeepAspectRatio, Qt::SmoothTransformation));
        ui->displayLabel->update();  // å¼ºåˆ¶æ›´æ–°
    }, Qt::QueuedConnection);
}

void eyeTrack::updateGazePlots(const cv::Point2f& gazePoint, const cv::Point2f& prediction, int frameId) {
    // æ›´æ–°çœŸå®æ³¨è§†ç‚¹å›¾è¡¨
    GazePointGraph->addData(gazePoint.x, gazePoint.y);
    GazePlot->rescaleAxes();
    GazePlot->replot();

    // æ›´æ–°é¢„æµ‹å›¾è¡¨ï¼ˆæ˜¾ç¤ºé¢„æµ‹è½¨è¿¹ï¼Œä¸æ˜¯æ»¤æ³¢è½¨è¿¹ï¼‰
    PredictPointGraph->addData(prediction.x, prediction.y);
    PredictPlot->rescaleAxes();
    PredictPlot->replot();

    // å®šæœŸæ¸…ç©ºæ•°æ®
    static int clearCounter = 0;
    if (++clearCounter > 500) {
        GazePointGraph->data()->clear();
        PredictPointGraph->data()->clear();
        clearCounter = 0;
        qDebug() << "å›¾è¡¨æ•°æ®å·²æ¸…ç©º";
    }
}
void eyeTrack::displayImageOnly(const cv::Mat& image) {
    displayImage(image);
}

void eyeTrack::acceptanceCoefficient(const std::vector<MappingCoefficients> &coefficients, const MappingCoefficients &coefficient)
{
    if(coefficients.size() == 0){
        // ä½¿ç”¨é¢„å®šä¹‰çš„é»˜è®¤æ˜ å°„ç³»æ•°
        initializeDefaultMappingCoefficients();

        // å¦‚æœä¼ å…¥çš„ç»„åˆç³»æ•°ä¹Ÿä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ç»„åˆç³»æ•°
        if(coefficient.xCoeff.empty() && coefficient.yCoeff.empty()) {
            combinedMappingCoefficients = m_mappingCoefficients[0]; // ä½¿ç”¨ç¬¬ä¸€ç»„ä½œä¸ºé»˜è®¤
        } else {
            combinedMappingCoefficients = coefficient;
        }

        qDebug() << "ä½¿ç”¨é»˜è®¤æ˜ å°„ç³»æ•°é…ç½®";
    } else {
        m_mappingCoefficients = coefficients;
        combinedMappingCoefficients = coefficient;
        qDebug() << "ä½¿ç”¨ä¼ å…¥çš„æ˜ å°„ç³»æ•°é…ç½®";
    }

    printceCoefficient(m_mappingCoefficients, combinedMappingCoefficients);
}

void eyeTrack::SaveCollectingData() {
    qDebug() << "=== å¼€å§‹ä¿å­˜æ•°æ® ===";
    qDebug() << "dataFlagçŠ¶æ€:" << dataFlag;
    qDebug() << "m_trueGazePointså¤§å°:" << m_trueGazePoints.size();
    qDebug() << "m_actualPredictionså¤§å°:" << m_actualPredictions.size();
    qDebug() << "m_nextFramePredictionså¤§å°:" << m_nextFramePredictions.size();

    // è¾“å‡ºä¸€äº›ç¤ºä¾‹æ•°æ®
    if (!m_trueGazePoints.empty()) {
        auto first = m_trueGazePoints.begin();
        auto last = m_trueGazePoints.rbegin();
        qDebug() << "æ•°æ®èŒƒå›´: å¸§" << first->first << "åˆ°å¸§" << last->first;
        qDebug() << "ç¬¬ä¸€å¸§æ•°æ®:" << first->second.x << "," << first->second.y;
        qDebug() << "æœ€åä¸€å¸§æ•°æ®:" << last->second.x << "," << last->second.y;
    }

    QString fileName = QDir::currentPath() + "/prediction_only_data.csv";
    qDebug() << "ä¿å­˜è·¯å¾„:" << fileName;

    QFile file(fileName);
    if (!file.open(QIODevice::WriteOnly | QIODevice::Text)) {
        qDebug() << "æ— æ³•å†™å…¥é¢„æµ‹æ•°æ®æ–‡ä»¶:" << fileName;
        qDebug() << "æ–‡ä»¶é”™è¯¯:" << file.errorString();
        return;
    }
    QTextStream out(&file);

    // ä¿®æ”¹CSVæ ¼å¼ï¼Œæ·»åŠ æ—¶é—´åˆ—å’Œæ³¨è§†ç‚¹ç›¸å…³ä¿¡æ¯
    out << "frameId,"
        << "actualX,"
        << "predictedX,"
        << "alphaBetaPredX,"     // æ–°å¢
        << "arxPredX,"           // æ–°å¢
        << "kalmanPredX,"        // æ–°å¢
        << "l2l3PredX,"          // L2+L3
        << "l1l2PredX,"          // L1+L2
        << "l1OnlyPredX,"        // ä»…L1

        // æ³¨è§†ç‚¹ç›¸å…³ä¿¡æ¯
        << "light1_x,light1_y,"  // å…‰æ–‘1åæ ‡
        << "light2_x,light2_y,"  // å…‰æ–‘2åæ ‡
        << "light3_x,light3_y,"  // å…‰æ–‘3åæ ‡
        << "light4_x,light4_y,"  // å…‰æ–‘4åæ ‡
        << "pupil_x,pupil_y,"    // ç³å­”ä¸­å¿ƒåæ ‡
        << "pupil_angle,"        // ç³å­”è§’åº¦
        << "pupil_area,"        // ç³å­”é¢ç§¯
        << "pupil_eccentricity,"  // ç³å­”åå¿ƒç‡
        << "pupil_Circularity,"   // ç³å­”ä¸­çº¿åç§»

        << "videoCaptureTime,"
        << "pupilTime,"
        << "roiTime,"
        << "spotTime,"
        << "predictTime,"
        << "DrawTime,"
        << "totalProcessTime\n";  // æ·»åŠ æ€»å¤„ç†æ—¶é—´

    int savedRecords = 0;

    // éå†æ‰€æœ‰å¸§
    for (const auto& pair : m_trueGazePoints) {
        int frameId = pair.first;
        cv::Point2f actualGaze = pair.second;

        out << frameId << ",";
        out << actualGaze.x << ",";

        // å¯¹ä¸‹ä¸€å¸§çš„é¢„æµ‹
        if (m_nextFramePredictions.find(frameId) != m_nextFramePredictions.end()) {
            cv::Point2f predForCurrentFrame = m_nextFramePredictions[frameId - 1];
            out << predForCurrentFrame.x << ",";
        } else {
            out << "NA,";
        }

        // AlphaBetaé¢„æµ‹å™¨Xè½´
        if (m_alphaBetaPredictionsX.find(frameId) != m_alphaBetaPredictionsX.end()) {
            out << m_alphaBetaPredictionsX[frameId] << ",";
        } else {
            out << "NA,";
        }

        // ARXé¢„æµ‹å™¨Xè½´
        if (m_arxPredictionsX.find(frameId) != m_arxPredictionsX.end()) {
            out << m_arxPredictionsX[frameId] << ",";
        } else {
            out << "NA,";
        }

        if (m_kalmanPredictionsX.find(frameId) != m_kalmanPredictionsX.end()) {
            out << m_kalmanPredictionsX[frameId] << ",";
        } else {
            out << "NA,";
        }

        if (m_l2l3PredictionsX.find(frameId) != m_l2l3PredictionsX.end()) {
            out << m_l2l3PredictionsX[frameId] << ",";
        } else {
            out << "NA,";
        }

        // L1+L2é¢„æµ‹å™¨Xè½´
        if (m_l1l2PredictionsX.find(frameId) != m_l1l2PredictionsX.end()) {
            out << m_l1l2PredictionsX[frameId] << ",";
        } else {
            out << "NA,";
        }

        // ä»…L1é¢„æµ‹å™¨Xè½´
        if (m_l1OnlyPredictionsX.find(frameId) != m_l1OnlyPredictionsX.end()) {
            out << m_l1OnlyPredictionsX[frameId] << ",";
        } else {
            out << "NA,";
        }

        // æ·»åŠ æ³¨è§†ç‚¹ç›¸å…³ä¿¡æ¯
        // å…‰æ–‘ç‚¹æ•°æ® (4ä¸ªå…‰æ–‘ï¼Œæ¯ä¸ªå…‰æ–‘x,yåæ ‡)
        if (lightTotal.find(frameId) != lightTotal.end()) {
            const auto& lightPoints = lightTotal[frameId];
            for (int i = 0; i < 4; i++) {
                if (i < lightPoints.size()) {
                    out << lightPoints[i].x << "," << lightPoints[i].y << ",";
                } else {
                    out << "NA,NA,";
                }
            }
        } else {
            out << "NA,NA,NA,NA,NA,NA,NA,NA,";  // 4ä¸ªå…‰æ–‘ï¼Œæ¯ä¸ª2ä¸ªåæ ‡
        }

        // ç³å­”ä¸­å¿ƒæ•°æ®
        if (pupilTotal.find(frameId) != pupilTotal.end()) {
            const auto& pupilCenter = pupilTotal[frameId];
            out << pupilCenter.x << "," << pupilCenter.y << ",";
        } else {
            out << "NA,NA,";
        }

        // ç³å­”è§’åº¦æ•°æ®
        if (angelTotal.find(frameId) != angelTotal.end()) {
            out << angelTotal[frameId] << ",";
        } else {
            out << "NA,";
        }

        // ç³å­”è§’åº¦é¢ç§¯
        if (areaTotal.find(frameId) != areaTotal.end()) {
            out << areaTotal[frameId] << ",";
        } else {
            out << "NA,";
        }

        // ç³å­”åå¿ƒç‡
        if (eccentricityTotal.find(frameId) != eccentricityTotal.end()) {
            out << eccentricityTotal[frameId] << ",";
        } else {
            out << "NA,";
        }

        // ç³å­”åœ†å½¢åº¦
        if (circularityTotal.find(frameId) != circularityTotal.end()) {
            out << circularityTotal[frameId] << ",";
        } else {
            out << "NA,";
        }

        // æ·»åŠ æ—¶é—´æ•°æ®
        // è§†é¢‘æ•è·æ—¶é—´
        if (videoCaptureTime.find(frameId) != videoCaptureTime.end()) {
            out << videoCaptureTime[frameId] << ",";
        } else {
            out << "NA,";
        }

        // ç³å­”æ£€æµ‹æ—¶é—´
        if (pupilTime.find(frameId) != pupilTime.end()) {
            out << pupilTime[frameId] << ",";
        } else {
            out << "NA,";
        }

        // ROIå¤„ç†æ—¶é—´
        if (roiTime.find(frameId) != roiTime.end()) {
            out << roiTime[frameId] << ",";
        } else {
            out << "NA,";
        }

        // å…‰æ–‘æ£€æµ‹æ—¶é—´
        if (spotTime.find(frameId) != spotTime.end()) {
            out << spotTime[frameId] << ",";
        } else {
            out << "NA,";
        }

        // é¢„æµ‹æ—¶é—´
        if (predictTime.find(frameId) != predictTime.end()) {
            out << predictTime[frameId] << ",";
        } else {
            out << "NA,";
        }

        if (DrawTime.find(frameId) != DrawTime.end()) {
            out << DrawTime[frameId] << ",";
        } else {
            out << "NA,";
        }

        // è®¡ç®—æ€»å¤„ç†æ—¶é—´ï¼ˆå¦‚æœæ‰€æœ‰æ—¶é—´æ•°æ®éƒ½å­˜åœ¨ï¼‰
        if (videoCaptureTime.find(frameId) != videoCaptureTime.end() &&
            pupilTime.find(frameId) != pupilTime.end() &&
            roiTime.find(frameId) != roiTime.end() &&
            spotTime.find(frameId) != spotTime.end() &&
            predictTime.find(frameId) != predictTime.end()) {

            double totalTime = videoCaptureTime[frameId] + pupilTime[frameId] +
                               roiTime[frameId] + spotTime[frameId] + predictTime[frameId] + DrawTime[frameId];
            out << totalTime;
        } else {
            out << "NA";
        }

        out << "\n";
        savedRecords++;

        // æ¯100æ¡è®°å½•è¾“å‡ºä¸€æ¬¡è¿›åº¦
        if (savedRecords % 100 == 0) {
            qDebug() << "å·²ä¿å­˜" << savedRecords << "æ¡è®°å½•";
        }
    }

    file.close();
    qDebug() << "çº¯é¢„æµ‹æ•°æ®ä¿å­˜å®Œæˆï¼";
    qDebug() << "æ€»å…±ä¿å­˜äº†" << savedRecords << "æ¡è®°å½•";
    qDebug() << "æ–‡ä»¶å¤§å°:" << QFileInfo(fileName).size() << "å­—èŠ‚";

    // è¾“å‡ºé¢„æµ‹æ€§èƒ½æ€»ç»“
    if (performanceStats.totalFrames > 0) {
        double avgError = performanceStats.horizontalErrorSum / performanceStats.totalFrames;
        double precision = (double)performanceStats.highPrecisionFrames / performanceStats.totalFrames * 100;
        qDebug() << QString("é¢„æµ‹æ€§èƒ½æ€»ç»“: å¹³å‡è¯¯å·®=%1 px, é«˜ç²¾åº¦ç‡=%2%%")
                        .arg(avgError, 0, 'f', 2)
                        .arg(precision, 0, 'f', 1);
    }

    // è¾“å‡ºæ—¶é—´æ€§èƒ½ç»Ÿè®¡
    if (!videoCaptureTime.empty()) {
        double avgVideoCaptureTime = 0, avgPupilTime = 0, avgRoiTime = 0, avgSpotTime = 0, avgPredictTime = 0;
        int validTimeCount = 0;

        for (const auto& pair : videoCaptureTime) {
            int frameId = pair.first;
            if (pupilTime.find(frameId) != pupilTime.end() &&
                roiTime.find(frameId) != roiTime.end() &&
                spotTime.find(frameId) != spotTime.end() &&
                predictTime.find(frameId) != predictTime.end()) {

                avgVideoCaptureTime += pair.second;
                avgPupilTime += pupilTime[frameId];
                avgRoiTime += roiTime[frameId];
                avgSpotTime += spotTime[frameId];
                avgPredictTime += predictTime[frameId];
                validTimeCount++;
            }
        }

        if (validTimeCount > 0) {
            avgVideoCaptureTime /= validTimeCount;
            avgPupilTime /= validTimeCount;
            avgRoiTime /= validTimeCount;
            avgSpotTime /= validTimeCount;
            avgPredictTime /= validTimeCount;

            double avgTotalTime = avgVideoCaptureTime + avgPupilTime + avgRoiTime + avgSpotTime + avgPredictTime;

            qDebug() << "=== æ—¶é—´æ€§èƒ½ç»Ÿè®¡ ===";
            qDebug() << QString("å¹³å‡è§†é¢‘æ•è·æ—¶é—´: %1 ms").arg(avgVideoCaptureTime, 0, 'f', 2);
            qDebug() << QString("å¹³å‡ç³å­”æ£€æµ‹æ—¶é—´: %1 ms").arg(avgPupilTime, 0, 'f', 2);
            qDebug() << QString("å¹³å‡ROIå¤„ç†æ—¶é—´: %1 ms").arg(avgRoiTime, 0, 'f', 2);
            qDebug() << QString("å¹³å‡å…‰æ–‘æ£€æµ‹æ—¶é—´: %1 ms").arg(avgSpotTime, 0, 'f', 2);
            qDebug() << QString("å¹³å‡é¢„æµ‹æ—¶é—´: %1 ms").arg(avgPredictTime, 0, 'f', 2);
            qDebug() << QString("å¹³å‡æ€»å¤„ç†æ—¶é—´: %1 ms").arg(avgTotalTime, 0, 'f', 2);
            qDebug() << QString("å¹³å‡FPS: %1").arg(1000.0 / avgTotalTime, 0, 'f', 1);
        }
    }
}
void eyeTrack::scanCreamDevice()
{
    //è·å–å¯ç”¨æ‘„åƒå¤´åˆ—è¡¨
    cameras = QMediaDevices::videoInputs();
    ui->comboBox->clear();
    //æ·»åŠ æ‘„åƒå¤´åˆ°ä¸‹æ‹‰åˆ—è¡¨
    for(const QCameraDevice &camera : cameras){
        qDebug()<<"adding camera:" <<camera.description();
        ui->comboBox->addItem(camera.description(), QVariant::fromValue(camera));
    }
    ui->comboBox->addItem("é€‰æ‹©æ–‡ä»¶", QString("file"));
}

void eyeTrack::on_StarPushButton_clicked()
{
    static bool isButtonProcessing = false;
    if (isButtonProcessing) {
        qDebug() << "æ£€æµ‹åˆ°é‡å¤ç‚¹å‡»ï¼Œå¿½ç•¥æ­¤æ¬¡è°ƒç”¨";
        return;
    }

    isButtonProcessing = true;
    QTimer::singleShot(3000, []() {
        isButtonProcessing = false;
        qDebug() << "æŒ‰é’®å¤„ç†æ ‡å¿—å·²è‡ªåŠ¨é‡ç½®";
    });

    if (currentState == STARTING || currentState == STOPPING) {
        qDebug() << "ç³»ç»Ÿæ­£åœ¨" << (currentState == STARTING ? "å¯åŠ¨" : "åœæ­¢") << "ä¸­ï¼Œè¯·ç¨å€™...";
        isButtonProcessing = false;
        return;
    }

    qDebug() << "on_StarPushButton_clicked - å¼€å§‹å¤„ç†";

    if (currentState == STOPPED) {
        qDebug() << "=== å¼€å§‹å¯åŠ¨ç³»ç»Ÿ ===";
        currentState = STARTING;
        cameraFlag = true;

        dataFlag = true;
        qDebug() << "æ•°æ®æ”¶é›†å·²å¯ç”¨ï¼ŒdataFlag=" << dataFlag;

        try {
            int index = ui->comboBox->currentIndex();
            if (index == -1) {
                qWarning() << "æœªé€‰æ‹©æ‘„åƒå¤´";
                currentState = STOPPED;
                cameraFlag = false;
                dataFlag = false;  // é‡ç½®æ ‡å¿—
                isButtonProcessing = false;
                return;
            }

            QVariant selectedItemData = ui->comboBox->itemData(index);
            qDebug() << "selectedItemData type:" << selectedItemData.typeName();
            qDebug() << "selectedItemData:" << selectedItemData.toString();

            if (selectedItemData.toString() == "file") {
                qDebug() << "å‡†å¤‡æ‰“å¼€æ–‡ä»¶å¯¹è¯æ¡†";
                QString filePath = QFileDialog::getOpenFileName(this, "é€‰æ‹©è§†é¢‘æ–‡ä»¶", "", "Videos (*.mp4 *.avi *.mjpeg)");
                qDebug() << "æ–‡ä»¶å¯¹è¯æ¡†å·²å…³é—­ï¼Œé€‰æ‹©çš„æ–‡ä»¶ï¼š" << filePath;

                if (filePath.isEmpty()) {
                    qWarning() << "æ–‡ä»¶ä¸ºç©º";
                    currentState = STOPPED;
                    cameraFlag = false;
                    dataFlag = false;  // é‡ç½®æ ‡å¿—
                    isButtonProcessing = false;
                    return;
                }
                cameraPipe->setSource(1, filePath);
            } else {
                QCameraDevice selectedCamera = selectedItemData.value<QCameraDevice>();
                qDebug() << "selectedCamera:" << selectedCamera.description();
                if (selectedCamera.isNull()) {
                    qWarning() << "é€‰æ‹©æ‘„åƒå¤´æ— æ•ˆ";
                    currentState = STOPPED;
                    cameraFlag = false;
                    dataFlag = false;  // é‡ç½®æ ‡å¿—
                    isButtonProcessing = false;
                    return;
                }
                cameraPipe->setSource(0, selectedCamera.description());
            }

            pip->creat_capturepip(cameraPipe, false);
            pip->add_process_modles(mergedPip);
            pip->createPipeLine();

            imageSave.setImageBufferEnable(true);

            if (timer) {
                timer->start(30);
            }

            currentState = RUNNING;
            this->ui->StarPushButton->setText("å…³é—­æ‘„åƒå¤´");
            performanceLabel->setText("ç³»ç»Ÿè¿è¡Œä¸­...");
            performanceLabel->setStyleSheet("color: green; font-weight: bold;");

            qDebug() << "=== ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼ŒdataFlag=" << dataFlag << " ===";

        } catch (const std::exception& e) {
            qCritical() << "å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸:" << e.what();
            currentState = STOPPED;
            cameraFlag = false;
            dataFlag = false;  // é‡ç½®æ ‡å¿—
            this->ui->StarPushButton->setText("å¼€å¯æ‘„åƒå¤´");
            performanceLabel->setText("å¯åŠ¨å¤±è´¥");
            performanceLabel->setStyleSheet("color: red; font-weight: bold;");
            isButtonProcessing = false;
        }
    }

    isButtonProcessing = false;
    qDebug() << "on_StarPushButton_clicked - å¤„ç†å®Œæˆ";
}


bool eyeTrack::isSystemRunning() const {
    return currentState == RUNNING;
}

bool eyeTrack::isSystemReady() const {
    return currentState == STOPPED;
}

void eyeTrack::on_OutPushButton_clicked()
{
    SaveCollectingData();
    dataFlag = 0;
    ui->displayLabel->clear();
}

void eyeTrack::on_OutSavePushButton_clicked()
{
    SaveCollectingData();
    dataFlag = 0;
    ui->displayLabel->clear();
    pip->pausePipeLine();
    imageSave.saveDisplayBufferImage(this);
    imageSave.saveOriginalBufferImage(this);

    imageSave.setImageBufferEnable(false);
}


void eyeTrack::saveInvalidFrameImage(const cv::Mat& image, int frameId, QString fileName) {
    QString timestamp = QDateTime::currentDateTime().toString("yyyyMMdd_hhmmss_zzz");
    QString filename = QString("%1_%2_%3.jpg")
                           .arg(fileName)
                           .arg(frameId)
                           .arg(timestamp);
    QString filepath = QString("./error_images/%1").arg(filename);

    // ç¡®ä¿ç›®å½•å­˜åœ¨
    QDir dir("./error_images");
    if (!dir.exists()) {
        dir.mkpath(".");
    }

    // ä¿å­˜å›¾åƒ
    if (cv::imwrite(filepath.toStdString(), image)) {
        qDebug() << "æ— æ•ˆå¸§å›¾åƒå·²ä¿å­˜:" << filepath;
    } else {
        qDebug() << "ä¿å­˜æ— æ•ˆå¸§å›¾åƒå¤±è´¥:" << filepath;
    }
}

void eyeTrack::chartUpdates(const cv::Point2f &gazePoint, const cv::Point2f &predictedPoint, int frameId) {
    // 1. æ›´æ–°å›¾è¡¨æ˜¾ç¤ºï¼ˆgazePointæ˜¯çœŸå®å€¼ï¼ŒpredictedPointæ˜¯é¢„æµ‹å€¼ï¼‰
    updateGazePlots(gazePoint, predictedPoint, frameId);

    // 2. éªŒè¯æ³¨è§†ç‚¹æœ‰æ•ˆæ€§
    if (!isGazePointValid(gazePoint)) {
        if (frameId % 30 == 0) {
            qWarning() << QString("å¸§%1: æ³¨è§†ç‚¹æ— æ•ˆ (%.2f, %.2f)").arg(frameId).arg(gazePoint.x).arg(gazePoint.y);
        }
        return;
    }

    // 3. ä½¿ç”¨é¢„æµ‹å€¼è¿›è¡ŒçŸ«æ­£
    cv::Point2f actualPredictionForCorrection;
    bool hasPrediction = false;

    // æ£€æŸ¥æ˜¯å¦æœ‰å¯¹å½“å‰å¸§çš„é¢„æµ‹
    if (m_actualPredictions.find(frameId) != m_actualPredictions.end()) {
        actualPredictionForCorrection = m_actualPredictions[frameId];
        hasPrediction = true;

        if (frameId % 10 == 0) {
            qDebug() << QString("ä½¿ç”¨é¢„æµ‹è¿›è¡ŒçŸ«æ­£ - å¸§%1: é¢„æµ‹=(%2,%3), çœŸå®=(%4,%5)")
                            .arg(frameId)
                            .arg(actualPredictionForCorrection.x, 0, 'f', 2)
                            .arg(actualPredictionForCorrection.y, 0, 'f', 2)
                            .arg(gazePoint.x, 0, 'f', 2)
                            .arg(gazePoint.y, 0, 'f', 2);
        }
    } else {
        // æ²¡æœ‰é¢„æµ‹æ—¶ä½¿ç”¨å½“å‰å€¼
        actualPredictionForCorrection = gazePoint;
        hasPrediction = false;
    }
    QElapsedTimer DrawTimer;
    DrawTimer.start();

    // 4. çŸ«æ­£å¤„ç†ï¼ˆä½¿ç”¨é¢„æµ‹-çœŸå®å·®å€¼è¿›è¡ŒçŸ«æ­£ï¼‰
    if (nystagmusSimulationActive) {
        qDebug()<<"éœ‡é¢¤";
        currentCorrectionMode = MODE_NYSTAGMUS_SIMULATION;
        processNystagmusSimulation(gazePoint, frameId);
    } else {
        qDebug()<<"çŸ«æ­£";
        currentCorrectionMode = MODE_NORMAL_CORRECTION;
        processNormalCorrection(gazePoint, actualPredictionForCorrection, frameId);
    }
    double Time = DrawTimer.nsecsElapsed() / 1e6;
    DrawTime[frameId] = Time;
}

bool eyeTrack::isGazePointValid(const cv::Point2f& gazePoint) {
    // æ£€æŸ¥NaNå’ŒInf
    if (std::isnan(gazePoint.x) || std::isnan(gazePoint.y) ||
        std::isinf(gazePoint.x) || std::isinf(gazePoint.y)) {
        return false;
    }

    // æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆå¯ä»¥ç¨å¾®è¶…å‡ºå›¾åƒè¾¹ç•Œï¼‰
    const float MARGIN = 500.0f;  // å…è®¸è¶…å‡ºå›¾åƒè¾¹ç•Œçš„èŒƒå›´
    if (gazePoint.x < -MARGIN || gazePoint.x > IMAGE_WIDTH + MARGIN ||
        gazePoint.y < -MARGIN || gazePoint.y > IMAGE_HEIGHT + MARGIN) {
        return false;
    }

    return true;
}
void eyeTrack::processNystagmusSimulation(const cv::Point2f &currentGazePoint, int frameId) {
    if (!hasGazeReference) {
        centerReference = currentGazePoint;
        lastGazePoint = currentGazePoint;
        hasGazeReference = true;
        qDebug() << QString("è®¾ç½®å‚è€ƒä¸­å¿ƒ: %1, %2").arg(centerReference.x, 0, 'f', 2).arg(centerReference.y, 0, 'f', 2);
        return;
    }

    cv::Point2f gazeOffset = currentGazePoint - centerReference;

    // æ¯å¸§éƒ½å¤„ç†å›¾åƒä½ç§» - å»é™¤é—´éš”å¤„ç†
    applyNystagmusDisplacement(gazeOffset);

    // ç»Ÿè®¡å¯ä»¥ä¿ç•™é—´éš”ï¼ˆä¸å½±å“å›¾åƒå¤„ç†ï¼‰
    if (frameId % 5 == 0) {
        simStats.updateStats(gazeOffset);
    }

    if (frameId % 60 == 0) {
        outputRealTimeNystagmusStats(currentGazePoint, gazeOffset, frameId);
    }

    if (frameId % 30 == 0) {
        // ä½¿ç”¨æ›¼å“ˆé¡¿è·ç¦»æ›¿ä»£æ¬§å‡ é‡Œå¾—è·ç¦»
        double magnitude = std::abs(gazeOffset.x) + std::abs(gazeOffset.y);
        qDebug() << QString("å¸§%1: å½“å‰=(%2,%3), åç§»=(%4,%5), å¹…åº¦=%6px")
                        .arg(frameId)
                        .arg(currentGazePoint.x, 0, 'f', 2)
                        .arg(currentGazePoint.y, 0, 'f', 2)
                        .arg(gazeOffset.x, 0, 'f', 2)
                        .arg(gazeOffset.y, 0, 'f', 2)
                        .arg(magnitude, 0, 'f', 2);
    }

    lastGazePoint = currentGazePoint;
}

void eyeTrack::outputRealTimeNystagmusStats(const cv::Point2f& currentGaze, const cv::Point2f& offset, int frameId) {
    QString status = QString("æ°´å¹³çœ¼éœ‡æ¨¡æ‹Ÿ[%1å¸§]: æ³¨è§†ç‚¹X=%2, Xåç§»=%3px, å¹³å‡=%4px, æœ€å¤§=%5px")
                         .arg(frameId)
                         .arg(currentGaze.x, 0, 'f', 1)
                         .arg(offset.x, 0, 'f', 1)
                         .arg(simStats.avgOffset, 0, 'f', 1)
                         .arg(simStats.maxOffset, 0, 'f', 1);

    performanceLabel->setText(status);
    qDebug() << status;
}

void eyeTrack::processNormalCorrection(const cv::Point2f &gazePoint, const cv::Point2f &predictedPoint, int frameId) {
    static int debugCounter = 0;
    debugCounter++;

    cv::Point2f displacement = calculateDisplacement(gazePoint, predictedPoint);

    // ä½¿ç”¨æ›¼å“ˆé¡¿è·ç¦»ç®€åŒ–è®¡ç®—
    double rawDisplacementMagnitude = std::abs(predictedPoint.x - gazePoint.x) + std::abs(predictedPoint.y - gazePoint.y);
    double finalDisplacementMagnitude = std::abs(displacement.x) + std::abs(displacement.y);

    applyTremorCorrection(displacement);

    // æ¯å¸§éƒ½æ›´æ–°å›¾åƒæ˜¾ç¤º - å»é™¤é—´éš”å¤„ç†
    updateCorrectedImageDisplay();

    // å‡å°‘ç»Ÿè®¡è®¡ç®—é¢‘ç‡ï¼ˆè¿™ä¸ªå¯ä»¥ä¿ç•™é—´éš”ï¼Œä¸å½±å“å›¾åƒå¤„ç†ï¼‰
    static double totalCorrectionError = 0;
    static int correctionCount = 0;
    if (frameId % 5 == 0) {
        totalCorrectionError += rawDisplacementMagnitude;
        correctionCount++;
    }

    if (correctionCount % 20 == 0 && debugCounter % 30 == 0) {
        double avgError = totalCorrectionError / correctionCount;
        qDebug() << QString("çŸ«æ­£ç»Ÿè®¡%1: å¹³å‡è¯¯å·®=%2 px").arg(correctionCount).arg(avgError, 0, 'f', 2);
    }
}


void eyeTrack::applyNystagmusDisplacement(const cv::Point2f& gazeOffset) {
    if (originalFieldImage.empty()) {
        return;
    }

    // ä½¿ç”¨1/4å°ºå¯¸çš„åŸå§‹å›¾åƒ
    cv::Mat smallOriginalImage;
    cv::Size processSize(originalFieldImage.cols / 4, originalFieldImage.rows / 4);
    cv::resize(originalFieldImage, smallOriginalImage, processSize, 0, 0, cv::INTER_LINEAR);

    cv::Mat displacedImage;
    cv::Point2f scaledOffset = gazeOffset * 0.25f;

    applyGazeBasedDisplacement(smallOriginalImage, displacedImage, scaledOffset);

    baseImage = displacedImage.clone();
    displayNystagmusImage(displacedImage, gazeOffset);
}


void eyeTrack::applyTremorCorrection(const Point2f &displacement)
{
    if (!correctionParams.enableCorrection) {
        return;
    }

    // æ›´æ–°å½“å‰åç§»é‡
    currentOffset = displacement;

    // åº”ç”¨å¹³æ»‘æ»¤æ³¢
    smoothOffset = smoothOffset * (1.0 - smoothingFactor) + currentOffset * smoothingFactor;

    // è®°å½•çŸ«æ­£æ•°æ®ç”¨äºåˆ†æ
    recordCorrectionData(displacement, smoothOffset);
}
cv::Point2f eyeTrack::calculateDisplacement(const cv::Point2f &gazePoint, const cv::Point2f &predictedPoint) {
    // éœ‡é¢¤çŸ«æ­£ï¼šè®¡ç®—é¢„æµ‹è¯¯å·®å¹¶åå‘è¡¥å¿
    // å¦‚æœé¢„æµ‹ç‚¹åœ¨çœŸå®ç‚¹å³è¾¹ï¼Œè¯´æ˜çœ¼ç›å°†å‘å³ç§»åŠ¨ï¼Œæˆ‘ä»¬éœ€è¦å°†å›¾åƒå‘å·¦ç§»åŠ¨
    cv::Point2f predictionError = predictedPoint - gazePoint;
    cv::Point2f displacement = -predictionError;  // åå‘ç§»åŠ¨ä»¥è¡¥å¿

    static int debugCount = 0;
    debugCount++;


    // åªå¤„ç†Xè½´
    displacement.y = 0;

    // åº”ç”¨æ­»åŒº
    if (std::abs(displacement.x) < correctionParams.deadZone) {
        displacement.x = 0;
    }

    // é™åˆ¶æœ€å¤§åç§»
    if (std::abs(displacement.x) > correctionParams.maxOffset) {
        float sign = (displacement.x > 0) ? 1.0f : -1.0f;
        displacement.x = sign * static_cast<float>(correctionParams.maxOffset);
    }

    // åº”ç”¨å¢ç›Š
    displacement.x *= static_cast<float>(correctionParams.gainFactor);

    return displacement;
}

void eyeTrack::visualizeDisplacement(cv::Mat& image, const cv::Point2f& displacement, const QString& mode) {
    // åœ¨å›¾åƒä¸Šç»˜åˆ¶ä½ç§»ä¿¡æ¯
    int centerX = image.cols / 2;
    int centerY = image.rows / 2;

    // ç»˜åˆ¶ä¸­å¿ƒç‚¹
    cv::circle(image, cv::Point(centerX, centerY), 5, cv::Scalar(0, 255, 0), -1);

    // ç»˜åˆ¶ä½ç§»å‘é‡
    if (cv::norm(displacement) > 1.0) {
        cv::Point endPoint(centerX + displacement.x * 5, centerY + displacement.y * 5);
        cv::arrowedLine(image, cv::Point(centerX, centerY), endPoint,
                        cv::Scalar(0, 0, 255), 2, 8, 0, 0.2);
    }

    // æ·»åŠ æ–‡å­—è¯´æ˜
    std::string modeText = mode.toStdString() + " Displacement: " +
                           std::to_string((int)displacement.x) + "px";
    cv::putText(image, modeText, cv::Point(10, 30),
                cv::FONT_HERSHEY_SIMPLEX, 0.7, cv::Scalar(255, 255, 0), 2);
}

void eyeTrack::updateCorrectedImageDisplay() {
    if (baseImage.empty()) {
        qDebug() << "åŸºç¡€å›¾åƒä¸ºç©º";
        return;
    }

    // ä½¿ç”¨1/4å°ºå¯¸å›¾åƒå¤„ç†
    cv::Mat smallBaseImage;
    cv::Size processSize(baseImage.cols / 4, baseImage.rows / 4);
    cv::resize(baseImage, smallBaseImage, processSize, 0, 0, cv::INTER_LINEAR);

    cv::Mat correctedImage;
    cv::Point2f scaledOffset = smoothOffset * 0.25f;

    applySpatialCorrection(smallBaseImage, correctedImage, scaledOffset);

    if (correctedImage.empty()) {
        qWarning() << "çŸ«æ­£å›¾åƒä¸ºç©º";
        return;
    }

    QImage qimg = matToQImage(correctedImage);
    if (qimg.isNull()) {
        qWarning() << "QImageè½¬æ¢å¤±è´¥";
        return;
    }

    QPixmap pixmap = QPixmap::fromImage(qimg);
    if (!ui->VideoLabel) {
        qWarning() << "VideoLabelä¸ºç©º";
        return;
    }

    QSize labelSize = ui->VideoLabel->size();
    if (labelSize.width() > 0 && labelSize.height() > 0) {
        // ä½¿ç”¨å¿«é€Ÿç¼©æ”¾
        pixmap = pixmap.scaled(labelSize, Qt::IgnoreAspectRatio, Qt::FastTransformation);
        ui->VideoLabel->setPixmap(pixmap);
    }
}



void eyeTrack::applyGazeBasedDisplacement(const cv::Mat& inputImage, cv::Mat& outputImage, const cv::Point2f& gazeOffset) {
    if (inputImage.empty()) {
        qDebug() << "è¾“å…¥å›¾åƒä¸ºç©º";
        return;
    }

    // åªå¤„ç†Xè½´åç§»ï¼Œæé«˜æ€§èƒ½
    int offsetX = static_cast<int>(std::round(-gazeOffset.x));

    // é™åˆ¶åç§»èŒƒå›´
    offsetX = std::max(-100, std::min(100, offsetX));

    outputImage = cv::Mat::zeros(inputImage.size(), inputImage.type());

    if (offsetX == 0) {
        inputImage.copyTo(outputImage);
        addNystagmusSimulationOverlay(outputImage, gazeOffset);
        return;
    }

    cv::Rect srcRect, dstRect;

    if (offsetX > 0) {
        srcRect = cv::Rect(0, 0, inputImage.cols - offsetX, inputImage.rows);
        dstRect = cv::Rect(offsetX, 0, srcRect.width, srcRect.height);
    } else {
        int absOffsetX = -offsetX;
        srcRect = cv::Rect(absOffsetX, 0, inputImage.cols - absOffsetX, inputImage.rows);
        dstRect = cv::Rect(0, 0, srcRect.width, srcRect.height);
    }

    // å¿«é€Ÿè¾¹ç•Œæ£€æŸ¥
    srcRect &= cv::Rect(0, 0, inputImage.cols, inputImage.rows);
    dstRect &= cv::Rect(0, 0, outputImage.cols, outputImage.rows);

    if (srcRect.width > 0 && srcRect.height > 0) {
        inputImage(srcRect).copyTo(outputImage(dstRect));
    }

    addNystagmusSimulationOverlay(outputImage, gazeOffset);
}

void eyeTrack::addNystagmusSimulationOverlay(cv::Mat& image, const cv::Point2f& gazeOffset) {
    // æ·»åŠ çœ¼éœ‡åç§»ä¿¡æ¯æ–‡å­—
    std::string offsetText = "Horizontal Nystagmus Offset: " +
                             std::to_string((int)gazeOffset.x) + "px";

    cv::putText(image, offsetText, cv::Point(10, image.rows - 80),
                cv::FONT_HERSHEY_SIMPLEX, 0.7, cv::Scalar(255, 255, 0), 2);

    std::string modeText = "Mode: Horizontal Nystagmus Simulation (X-axis Only)";
    cv::putText(image, modeText, cv::Point(10, image.rows - 50),
                cv::FONT_HERSHEY_SIMPLEX, 0.6, cv::Scalar(255, 255, 0), 2);

    // æ˜¾ç¤ºå‚è€ƒç‚¹ä¿¡æ¯
    std::string refText = "Reference: Image Center (" +
                          std::to_string((int)imageCenterReference.x) + ", " +
                          std::to_string((int)imageCenterReference.y) + ")";
    cv::putText(image, refText, cv::Point(10, image.rows - 110),
                cv::FONT_HERSHEY_SIMPLEX, 0.6, cv::Scalar(255, 255, 0), 2);

    // ç»Ÿè®¡ä¿¡æ¯
    std::string statsText = "Avg: " + std::to_string((int)simStats.avgOffset) + "px | " +
                            "Max: " + std::to_string((int)simStats.maxOffset) + "px | " +
                            "Frames: " + std::to_string(simStats.totalFrames);
    cv::putText(image, statsText, cv::Point(10, image.rows - 20),
                cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(255, 255, 0), 1);

    // ç»˜åˆ¶å›¾åƒä¸­å¿ƒå‚è€ƒç‚¹ï¼ˆå›ºå®šä½ç½®ï¼‰
    int centerX = (int)imageCenterReference.x;
    int centerY = (int)imageCenterReference.y;

    // ç¡®ä¿ä¸­å¿ƒç‚¹åœ¨å›¾åƒèŒƒå›´å†…
    if (centerX >= 0 && centerX < image.cols && centerY >= 0 && centerY < image.rows) {
        // ç»˜åˆ¶ä¸­å¿ƒåå­—ï¼ˆè¾ƒå¤§ï¼Œä¾¿äºè¯†åˆ«ï¼‰
        cv::line(image, cv::Point(centerX-20, centerY), cv::Point(centerX+20, centerY),
                 cv::Scalar(0, 255, 255), 3);
        cv::line(image, cv::Point(centerX, centerY-20), cv::Point(centerX, centerY+20),
                 cv::Scalar(0, 255, 255), 3);

        // åœ¨ä¸­å¿ƒç‚¹é™„è¿‘æ·»åŠ æ–‡å­—æ ‡è¯†
        cv::putText(image, "CENTER", cv::Point(centerX-30, centerY-25),
                    cv::FONT_HERSHEY_SIMPLEX, 0.6, cv::Scalar(0, 255, 255), 2);

        // ç»˜åˆ¶æ°´å¹³åç§»å‘é‡ï¼ˆåªåœ¨Xæ–¹å‘ï¼‰
        if (std::abs(gazeOffset.x) > 3.0) {
            cv::Point offsetEnd(centerX + gazeOffset.x, centerY);  // Yåæ ‡ä¿æŒä¸å˜

            // ç¡®ä¿ç®­å¤´ç»ˆç‚¹åœ¨å›¾åƒèŒƒå›´å†…
            offsetEnd.x = std::max(0, std::min(image.cols - 1, offsetEnd.x));

            cv::arrowedLine(image, cv::Point(centerX, centerY), offsetEnd,
                            cv::Scalar(0, 0, 255), 3, 8, 0, 0.3);
        }
    }

    // ç»˜åˆ¶æ°´å¹³åç§»è½¨è¿¹
    drawHorizontalGazeOffsetTrajectory(image, centerX, centerY);
}


void eyeTrack::drawHorizontalGazeOffsetTrajectory(cv::Mat& image, int centerX, int centerY) {
    // ç»˜åˆ¶æœ€è¿‘çš„æ°´å¹³åç§»è½¨è¿¹
    if (simStats.recentOffsets.size() > 1) {
        for (size_t i = 1; i < simStats.recentOffsets.size(); i++) {
            // åªä½¿ç”¨Xåç§»ï¼ŒYåæ ‡å›ºå®šåœ¨ä¸­å¿ƒ
            cv::Point p1(centerX + simStats.recentOffsets[i-1].x, centerY);
            cv::Point p2(centerX + simStats.recentOffsets[i].x, centerY);

            // æ¸å˜è‰²å½©ï¼Œæœ€æ–°çš„æ›´äº®
            double alpha = (double)i / simStats.recentOffsets.size();
            cv::Scalar color(0, 255 * alpha, 255 * alpha);
            cv::line(image, p1, p2, color, 2);
        }
    }
}
void eyeTrack::displayNystagmusImage(const cv::Mat& displacedImage, const cv::Point2f& gazeOffset) {
    QImage qimg = matToQImage(displacedImage);
    if (qimg.isNull()) {
        return;
    }

    QPixmap pixmap = QPixmap::fromImage(qimg);
    if (!ui->VideoLabel) {
        return;
    }

    QSize labelSize = ui->VideoLabel->size();
    if (labelSize.width() > 0 && labelSize.height() > 0) {
        // ä½¿ç”¨æœ€å¿«çš„ç¼©æ”¾æ–¹å¼
        pixmap = pixmap.scaled(labelSize, Qt::IgnoreAspectRatio, Qt::FastTransformation);
        ui->VideoLabel->setPixmap(pixmap);
    }
}
void eyeTrack::addQtNystagmusOverlay(QPixmap& pixmap, const cv::Point2f& gazeOffset) {
    QPainter painter(&pixmap);
    painter.setPen(QPen(Qt::yellow, 2));
    painter.setFont(QFont("Arial", 12, QFont::Bold));

    // æ˜¾ç¤ºå½“å‰æ°´å¹³åç§»ä¿¡æ¯
    QString offsetInfo = QString("Horizontal Offset: %1px")
                             .arg(gazeOffset.x, 0, 'f', 1);
    painter.drawText(10, 25, offsetInfo);

    // æ˜¾ç¤ºæ¨¡æ‹ŸçŠ¶æ€
    QString statusInfo = "Horizontal Nystagmus Simulation";
    painter.setPen(QPen(Qt::red, 2));
    painter.drawText(10, 50, statusInfo);

    // æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    QString statsInfo = QString("Avg: %1px | Max: %2px")
                            .arg(simStats.avgOffset, 0, 'f', 1)
                            .arg(simStats.maxOffset, 0, 'f', 1);
    painter.setPen(QPen(Qt::cyan, 2));
    painter.drawText(10, 75, statsInfo);
}


void eyeTrack::drawGazeOffsetTrajectory(cv::Mat& image, int centerX, int centerY) {
    // ç»˜åˆ¶æœ€è¿‘çš„åç§»è½¨è¿¹
    if (simStats.recentOffsets.size() > 1) {
        for (size_t i = 1; i < simStats.recentOffsets.size(); i++) {
            cv::Point p1(centerX + simStats.recentOffsets[i-1].x, centerY + simStats.recentOffsets[i-1].y);
            cv::Point p2(centerX + simStats.recentOffsets[i].x, centerY + simStats.recentOffsets[i].y);

            // æ¸å˜è‰²å½©ï¼Œæœ€æ–°çš„æ›´äº®
            double alpha = (double)i / simStats.recentOffsets.size();
            cv::Scalar color(0, 255 * alpha, 255 * alpha);
            cv::line(image, p1, p2, color, 2);
        }
    }
}
void eyeTrack::applySpatialCorrection(const cv::Mat& inputImage, cv::Mat& outputImage, const cv::Point2f& offset) {
    if (inputImage.empty()) {
        qDebug() << "è¾“å…¥å›¾åƒä¸ºç©º";
        return;
    }

    // ç®€åŒ–ä¸ºæ•´æ•°åç§»ï¼Œé¿å…æµ®ç‚¹è¿ç®—
    int offsetX = static_cast<int>(std::round(offset.x));
    int offsetY = static_cast<int>(std::round(offset.y));

    // é™åˆ¶åç§»èŒƒå›´ä»¥é¿å…è¿‡å¤§è®¡ç®—
    offsetX = std::max(-50, std::min(50, offsetX));
    offsetY = std::max(-50, std::min(50, offsetY));

    // ç›´æ¥ä½¿ç”¨é›¶åˆå§‹åŒ–ï¼Œé¿å…å¤æ‚å¡«å……
    outputImage = cv::Mat::zeros(inputImage.size(), inputImage.type());

    if (offsetX == 0 && offsetY == 0) {
        inputImage.copyTo(outputImage);
        return;
    }

    cv::Rect srcRect, dstRect;

    // ç®€åŒ–è¾¹ç•Œè®¡ç®—
    if (offsetX >= 0 && offsetY >= 0) {
        srcRect = cv::Rect(0, 0, inputImage.cols - offsetX, inputImage.rows - offsetY);
        dstRect = cv::Rect(offsetX, offsetY, srcRect.width, srcRect.height);
    } else if (offsetX < 0 && offsetY >= 0) {
        srcRect = cv::Rect(-offsetX, 0, inputImage.cols + offsetX, inputImage.rows - offsetY);
        dstRect = cv::Rect(0, offsetY, srcRect.width, srcRect.height);
    } else if (offsetX >= 0 && offsetY < 0) {
        srcRect = cv::Rect(0, -offsetY, inputImage.cols - offsetX, inputImage.rows + offsetY);
        dstRect = cv::Rect(offsetX, 0, srcRect.width, srcRect.height);
    } else {
        srcRect = cv::Rect(-offsetX, -offsetY, inputImage.cols + offsetX, inputImage.rows + offsetY);
        dstRect = cv::Rect(0, 0, srcRect.width, srcRect.height);
    }

    // å¿«é€Ÿè¾¹ç•Œæ£€æŸ¥
    srcRect &= cv::Rect(0, 0, inputImage.cols, inputImage.rows);
    dstRect &= cv::Rect(0, 0, outputImage.cols, outputImage.rows);

    if (srcRect.width > 0 && srcRect.height > 0) {
        inputImage(srcRect).copyTo(outputImage(dstRect));
    }
}


void eyeTrack::handleBoundaryEffects(Mat &image, const Point2f &offset)
{
    // å¦‚æœåç§»è¾ƒå¤§ï¼Œå¯ä»¥é€šè¿‡å¡«å……æˆ–é•œåƒè¾¹ç•Œæ¥å‡å°‘é»‘è¾¹
    if (cv::norm(offset) > 10.0) {
        cv::copyMakeBorder(image, image,
                           abs(offset.y), abs(offset.y),
                           abs(offset.x), abs(offset.x),
                           cv::BORDER_REFLECT);
    }
}

void eyeTrack::addCorrectionOverlay(QPixmap &pixmap)
{
    QPainter painter(&pixmap);
    painter.setPen(QPen(Qt::green, 2));
    painter.setFont(QFont("Arial", 12));

    // æ˜¾ç¤ºå½“å‰åç§»ä¿¡æ¯
    QString offsetInfo = QString("Offset: (%1, %2)")
                             .arg(smoothOffset.x, 0, 'f', 2)
                             .arg(smoothOffset.y, 0, 'f', 2);        painter.drawText(10, 25, offsetInfo);

    // æ˜¾ç¤ºçŸ«æ­£çŠ¶æ€
    QString statusInfo = correctionParams.enableCorrection ? "Correction: ON" : "Correction: OFF";
    painter.drawText(10, 45, statusInfo);

    // ç»˜åˆ¶çŸ«æ­£å‘é‡
    if (cv::norm(smoothOffset) > correctionParams.deadZone) {
        int centerX = pixmap.width() / 2;
        int centerY = pixmap.height() / 2;

        painter.setPen(QPen(Qt::red, 3));
        painter.drawLine(centerX, centerY,
                         centerX - smoothOffset.x * 5,
                         centerY - smoothOffset.y * 5);

        // ç»˜åˆ¶ç®­å¤´
        drawArrow(painter, QPoint(centerX, centerY),
                  QPoint(centerX - smoothOffset.x * 5, centerY - smoothOffset.y * 5));
    }
}

void eyeTrack::drawArrow(QPainter &painter, const QPoint &start, const QPoint &end)
{
    painter.drawLine(start, end);

    // è®¡ç®—ç®­å¤´å¤´éƒ¨
    double angle = atan2((end.y() - start.y()), (end.x() - start.x()));
    int arrowLength = 10;
    double arrowAngle = M_PI / 6;

    QPoint arrowP1(
        end.x() - arrowLength * cos(angle - arrowAngle),
        end.y() - arrowLength * sin(angle - arrowAngle)
        );

    QPoint arrowP2(
        end.x() - arrowLength * cos(angle + arrowAngle),
        end.y() - arrowLength * sin(angle + arrowAngle)
        );

    painter.drawLine(end, arrowP1);
    painter.drawLine(end, arrowP2);
}

QImage eyeTrack::matToQImage(const Mat &mat)
{
    switch (mat.type()) {
    case CV_8UC4: {
        QImage image(mat.data, mat.cols, mat.rows, mat.step, QImage::Format_ARGB32);
        return image.rgbSwapped();
    }
    case CV_8UC3: {
        QImage image(mat.data, mat.cols, mat.rows, mat.step, QImage::Format_RGB888);
        return image.rgbSwapped();
    }
    case CV_8UC1: {
        QImage image(mat.data, mat.cols, mat.rows, mat.step, QImage::Format_Grayscale8);
        return image;
    }
    }
    return QImage();
}

void eyeTrack::recordCorrectionData(const Point2f &rawOffset, const Point2f &smoothedOffset)
{
    QMutexLocker locker(&m_dataStorageMutex);

    // å¯ä»¥å°†æ•°æ®ä¿å­˜åˆ°æˆå‘˜å˜é‡ä¸­ç”¨äºåç»­åˆ†æ
    struct CorrectionData {
        double timestamp;
        cv::Point2f rawOffset;
        cv::Point2f smoothedOffset;
        double correctionMagnitude;
    };

    static std::vector<CorrectionData> correctionHistory;

    CorrectionData data;
    data.timestamp = QEtimer.elapsed();
    data.rawOffset = rawOffset;
    data.smoothedOffset = smoothedOffset;
    data.correctionMagnitude = cv::norm(smoothedOffset);

    correctionHistory.push_back(data);

    // é™åˆ¶å†å²æ•°æ®å¤§å°
    if (correctionHistory.size() > 1000) {
        correctionHistory.erase(correctionHistory.begin());
    }
}

void eyeTrack::setCorrectionParameters(double gainFactor, double maxOffset, double deadZone, double smoothingFactor)
{
    correctionParams.gainFactor = gainFactor;
    correctionParams.maxOffset = maxOffset;
    correctionParams.deadZone = deadZone;
    this->smoothingFactor = smoothingFactor;
}

void eyeTrack::enableCorrection(bool enable)
{
    correctionParams.enableCorrection = enable;

    if (!enable) {
        // é‡ç½®åç§»é‡
        currentOffset = cv::Point2f(0, 0);
        smoothOffset = cv::Point2f(0, 0);

        // æ˜¾ç¤ºåŸå§‹å›¾åƒ
        if (!baseImage.empty()) {
            QImage qimg = matToQImage(baseImage);
            QPixmap pixmap = QPixmap::fromImage(qimg);
            pixmap = pixmap.scaled(ui->VideoLabel->size(), Qt::KeepAspectRatio, Qt::SmoothTransformation);
            ui->VideoLabel->setPixmap(pixmap);
        }
    }
}

void eyeTrack::startRealNystagmusSimulation() {
    qDebug() << QString("å¼€å§‹åŸºäºå›¾åƒä¸­å¿ƒçš„æ°´å¹³çœ¼éœ‡è§†é‡æ¨¡æ‹Ÿ - å‚è€ƒç‚¹(%.0f, %.0f)")
                    .arg(imageCenterReference.x)
                    .arg(imageCenterReference.y);

    // ä¿å­˜åŸå§‹èƒŒæ™¯å›¾åƒ
    if (!fieldImage.empty()) {
        originalFieldImage = fieldImage.clone();

        // éªŒè¯èƒŒæ™¯å›¾åƒå°ºå¯¸
        if (fieldImage.cols != IMAGE_WIDTH || fieldImage.rows != IMAGE_HEIGHT) {
            qDebug() << QString("èƒŒæ™¯å›¾åƒå°ºå¯¸: %1x%2, æœŸæœ›å°ºå¯¸: %3x%4")
                            .arg(fieldImage.cols).arg(fieldImage.rows)
                            .arg(IMAGE_WIDTH).arg(IMAGE_HEIGHT);

            // å¦‚æœå°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´å‚è€ƒç‚¹
            if (fieldImage.cols > 0 && fieldImage.rows > 0) {
                cv::Point2f adjustedCenter(fieldImage.cols / 2.0f, fieldImage.rows / 2.0f);
                qDebug() << QString("ä½¿ç”¨å®é™…å›¾åƒä¸­å¿ƒ: (%.0f, %.0f)").arg(adjustedCenter.x).arg(adjustedCenter.y);
                imageCenterReference = adjustedCenter;
            }
        }
    } else {
        qWarning() << "èƒŒæ™¯å›¾åƒä¸ºç©ºï¼Œæ— æ³•å¼€å§‹æ¨¡æ‹Ÿ";
        return;
    }

    // é‡ç½®æ¨¡æ‹Ÿç»Ÿè®¡
    simStats.reset();

    // æ›´æ–°UI
    ui->NystagmusSimulation->setText("åœæ­¢æ°´å¹³çœ¼éœ‡æ¨¡æ‹Ÿ");

    // æ˜¾ç¤ºæ¨¡æ‹Ÿä¿¡æ¯
    QString simInfo = QString("æ°´å¹³çœ¼éœ‡è§†é‡æ¨¡æ‹Ÿå·²å¯åŠ¨ - å›¾åƒä¸­å¿ƒ(%.0f, %.0f)")
                          .arg(imageCenterReference.x)
                          .arg(imageCenterReference.y);
    performanceLabel->setText(simInfo);
    performanceLabel->setStyleSheet("color: orange; font-weight: bold; background-color: rgba(255,165,0,0.2); padding: 5px;");

    qDebug() << simInfo;
}

void eyeTrack::stopRealNystagmusSimulation() {
    qDebug() << "åœæ­¢çœ¼éœ‡è§†é‡æ¨¡æ‹Ÿ";

    // æ¢å¤åŸå§‹èƒŒæ™¯å›¾åƒå’ŒçŸ«æ­£ç³»ç»Ÿ
    if (!originalFieldImage.empty()) {
        fieldImage = originalFieldImage.clone();
        baseImage = fieldImage.clone();
        image = fieldImage.clone();
    }

    // é‡æ–°å¯ç”¨é¢„æµ‹çŸ«æ­£ç³»ç»Ÿ
    correctionParams.enableCorrection = true;

    // æ›´æ–°æ˜¾ç¤º
    updateCorrectedImageDisplay();

    // é‡ç½®UI
    ui->NystagmusSimulation->setText("çœ¼éœ‡è§†é‡æ¨¡æ‹Ÿ");
    QString statusInfo = QString("çœ¼éœ‡æ¨¡æ‹Ÿå·²åœæ­¢ - ä½¿ç”¨å›¾åƒä¸­å¿ƒå‚è€ƒç‚¹(%.0f, %.0f)")
                             .arg(imageCenterReference.x)
                             .arg(imageCenterReference.y);
    performanceLabel->setText(statusInfo);
    performanceLabel->setStyleSheet("color: green; font-weight: bold; background-color: rgba(0,0,0,0.1); padding: 5px;");

    // è¾“å‡ºæ¨¡æ‹Ÿç»Ÿè®¡
    outputNystagmusSimulationStats();
}

cv::Point2f eyeTrack::getGazeOffsetFromImageCenter(const cv::Point2f& gazePoint) {
    return gazePoint - imageCenterReference;
}

void eyeTrack::displayReferencePointInfo() {
    qDebug() << "=== å‚è€ƒç‚¹ä¿¡æ¯ ===";
    qDebug() << QString("å›¾åƒå°ºå¯¸: %1 x %2").arg(IMAGE_WIDTH).arg(IMAGE_HEIGHT);
    qDebug() << QString("å›¾åƒä¸­å¿ƒå‚è€ƒç‚¹: (%.0f, %.0f)").arg(imageCenterReference.x).arg(imageCenterReference.y);
    qDebug() << QString("å½“å‰æ¨¡å¼: %1").arg(currentCorrectionMode == MODE_NYSTAGMUS_SIMULATION ? "çœ¼éœ‡æ¨¡æ‹Ÿ" : "é¢„æµ‹çŸ«æ­£");

    if (!fieldImage.empty()) {
        qDebug() << QString("å®é™…èƒŒæ™¯å›¾åƒå°ºå¯¸: %1 x %2").arg(fieldImage.cols).arg(fieldImage.rows);
    }
}
void eyeTrack::outputNystagmusSimulationStats() {
    qDebug() << "=== æ°´å¹³çœ¼éœ‡è§†é‡æ¨¡æ‹Ÿç»Ÿè®¡æŠ¥å‘Š ===";
    qDebug() << QString("æ€»å¸§æ•°: %1 å¸§").arg(simStats.totalFrames);
    qDebug() << QString("å¹³å‡Xè½´åç§»: %1 åƒç´ ").arg(simStats.avgOffset, 0, 'f', 2);
    qDebug() << QString("æœ€å¤§Xè½´åç§»: %1 åƒç´ ").arg(simStats.maxOffset, 0, 'f', 2);
    qDebug() << QString("æ¨¡æ‹Ÿæ¨¡å¼: åŸºäºçœŸå®æ³¨è§†ç‚¹çš„æ°´å¹³è§†é‡éœ‡é¢¤");

    // åœ¨UIä¸­æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    QString finalStats = QString("æ°´å¹³çœ¼éœ‡æ¨¡æ‹Ÿå®Œæˆ: %1å¸§, å¹³å‡Xåç§»%2px, æœ€å¤§Xåç§»%3px")
                             .arg(simStats.totalFrames)
                             .arg(simStats.avgOffset, 0, 'f', 1)
                             .arg(simStats.maxOffset, 0, 'f', 1);
    performanceLabel->setText(finalStats);

    // è®¡ç®—ä¸€äº›æœ‰æ„æ€çš„ç»Ÿè®¡
    if (!simStats.offsetMagnitudes.empty()) {
        // è®¡ç®—åç§»åˆ†å¸ƒ
        int smallOffsets = 0, mediumOffsets = 0, largeOffsets = 0;
        for (double mag : simStats.offsetMagnitudes) {
            if (mag < 10.0) smallOffsets++;
            else if (mag < 30.0) mediumOffsets++;
            else largeOffsets++;
        }

        double total = simStats.offsetMagnitudes.size();
        qDebug() << QString("Xè½´åç§»åˆ†å¸ƒ: å°(<10px)=%.1f%%, ä¸­(10-30px)=%.1f%%, å¤§(>30px)=%.1f%%")
                        .arg(smallOffsets/total*100)
                        .arg(mediumOffsets/total*100)
                        .arg(largeOffsets/total*100);
    }
}
void eyeTrack::on_NystagmusSimulation_clicked()
{
    nystagmusSimulationActive = !nystagmusSimulationActive;
    if (nystagmusSimulationActive) {  // æ”¹ä¸ºï¼šå½“ä¸ºtrueæ—¶å¯åŠ¨æ¨¡æ‹Ÿ
        qDebug()<<"å¯åŠ¨çœ¼éœ‡æ¨¡æ‹Ÿ"<<nystagmusSimulationActive;
        startRealNystagmusSimulation();
        ui->NystagmusSimulation->setText("åœæ­¢çœ¼éœ‡æ¨¡æ‹Ÿ");  // æŒ‰é’®æ˜¾ç¤º"åœæ­¢"
    } else {
        // å½“ä¸ºfalseæ—¶åœæ­¢æ¨¡æ‹Ÿ
        qDebug()<<"åœæ­¢çœ¼éœ‡æ¨¡æ‹Ÿ"<<nystagmusSimulationActive;
        stopRealNystagmusSimulation();
        ui->NystagmusSimulation->setText("å¼€å§‹çœ¼éœ‡æ¨¡æ‹Ÿ");  // æŒ‰é’®æ˜¾ç¤º"å¼€å§‹"
    }
}

cv::Point2f eyeTrack::applyAsymmetryCorrection(const cv::Point2f& basePrediction,
                                               const cv::Point2f& currentMeasurement,
                                               int frameId) {
    cv::Point2f corrected = basePrediction;

    // å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œç›´æ¥è¿”å›
    if (!hasLastMeasurement) {
        lastValidMeasurement = currentMeasurement;
        hasLastMeasurement = true;
        return basePrediction;
    }

    // è®¡ç®—é€Ÿåº¦
    cv::Point2f velocity = currentMeasurement - lastValidMeasurement;

    //Xè½´éå¯¹ç§°æ€§ä¿®æ­£
    if (velocity.x < -50) {  // é«˜é€Ÿè´Ÿå‘è¿åŠ¨
        qDebug()<<"Xè½´éå¯¹ç§°æ€§ä¿®æ­£";
        // æ ¹æ®æ•°æ®åˆ†æï¼Œè¯¯å·®çº¦ç­‰äº -0.9 * velocity
        float compensation = std::abs(velocity.x) * 0.9f;
        corrected.x -= compensation;  // è¿›ä¸€æ­¥å‘è´Ÿæ–¹å‘é¢„æµ‹

        if (frameId % 10 == 0 || std::abs(velocity.x) > 100) {
            qDebug() << QString("éå¯¹ç§°ä¿®æ­£ - Xè½´: é€Ÿåº¦=%1, è¡¥å¿=%2, åŸé¢„æµ‹=%3, ä¿®æ­£å=%4")
                            .arg(velocity.x, 0, 'f', 1)
                            .arg(compensation, 0, 'f', 1)
                            .arg(basePrediction.x, 0, 'f', 1)
                            .arg(corrected.x, 0, 'f', 1);
        }
    }


    // æ›´æ–°å†å²
    lastValidMeasurement = currentMeasurement;

    // è¾¹ç•Œæ£€æŸ¥
    corrected.x = std::max(0.0f, std::min(1920.0f, corrected.x));
    corrected.y = std::max(0.0f, std::min(1080.0f, corrected.y));

    return corrected;
}


bool eyeTrack::detectSimplePeak(const cv::Point2f& currentGazePoint, int frameId) {
    // éœ€è¦è‡³å°‘3ä¸ªå†å²ç‚¹
    static std::deque<cv::Point2f> recentPositions;
    static std::deque<int> recentFrames;

    // æ·»åŠ å½“å‰ä½ç½®
    recentPositions.push_back(currentGazePoint);
    recentFrames.push_back(frameId);

    // åªä¿ç•™æœ€è¿‘3ä¸ªä½ç½®
    if (recentPositions.size() > 3) {
        recentPositions.pop_front();
        recentFrames.pop_front();
    }

    // éœ€è¦3ä¸ªç‚¹æ‰èƒ½æ£€æµ‹å³°å€¼
    if (recentPositions.size() < 3) {
        return false;
    }

    cv::Point2f pos1 = recentPositions[0];  // æœ€æ—©çš„ç‚¹
    cv::Point2f pos2 = recentPositions[1];  // ä¸­é—´çš„ç‚¹ï¼ˆæ½œåœ¨å³°å€¼ï¼‰
    cv::Point2f pos3 = recentPositions[2];  // å½“å‰ç‚¹

    // === ç®€å•çš„å³°å€¼æ£€æµ‹æ¡ä»¶ ===

    // 1. æ£€æµ‹Xè½´çš„å³°å€¼ï¼špos2.x æ˜¯å±€éƒ¨æœ€å¤§å€¼
    bool isXPeak = (pos2.x > pos1.x) && (pos2.x > pos3.x);

    // 2. ç¡®ä¿å³°å€¼è¶³å¤Ÿæ˜æ˜¾ï¼ˆé¿å…å™ªå£°ï¼‰
    float leftRise = pos2.x - pos1.x;   // ä¸Šå‡å¹…åº¦
    float rightFall = pos2.x - pos3.x;  // ä¸‹é™å¹…åº¦

    bool significantPeak = (leftRise > 10.0f) && (rightFall > 10.0f);

    // 3. Xä½ç½®åˆç†æ€§æ£€æŸ¥
    bool validPosition = pos2.x > 550.0f;

    // 4. æ—¶é—´é—´éš”æ£€æŸ¥
    int actualPeakFrame = recentFrames[1];  // å³°å€¼åœ¨ä¸­é—´å¸§
    bool validInterval = (actualPeakFrame - peakInfo.lastPeakFrame) > 5;

    // æ‰€æœ‰æ¡ä»¶æ»¡è¶³æ‰ç¡®è®¤å³°å€¼
    if (isXPeak && significantPeak && validPosition && validInterval) {

        // æ›´æ–°å³°å€¼ä¿¡æ¯
        peakInfo.lastPeakFrame = actualPeakFrame;
        peakInfo.lastPeakPosition = pos2;
        peakInfo.totalPeaksDetected++;

        // æ ¹æ®Xä½ç½®å†³å®šè¡¥å¿å¸§æ•°
        if (pos2.x > 650.0f) {
            peakInfo.compensationFrameCount = 2;
        } else {
            peakInfo.compensationFrameCount = 1;
        }

        // å¯åŠ¨è¡¥å¿
        peakInfo.compensationActive = true;
        peakInfo.compensationStartFrame = frameId;

        qDebug() << QString(" ç®€å•å³°å€¼æ£€æµ‹[å¸§%1]: å³°å€¼å¸§=%2, ä½ç½®(%3f,%4f), ä¸Šå‡=%5f, ä¸‹é™=%6f")
                        .arg(frameId).arg(actualPeakFrame)
                        .arg(pos2.x).arg(pos2.y)
                        .arg(leftRise).arg(rightFall);

        QString peakMsg = QString(" å³°å€¼[å¸§%1]: Xè½´ä»%2é™åˆ°%3f, è¡¥å¿%4å¸§")
                              .arg(actualPeakFrame)
                              .arg(pos2.x).arg(pos3.x)
                              .arg(peakInfo.compensationFrameCount);


        return true;
    }

    return false;
}

void eyeTrack::initializeDefaultMappingCoefficients() {
    m_mappingCoefficients.clear();
    m_mappingCoefficients.resize(4);

    // å¯ä»¥ä»é…ç½®æ–‡ä»¶æˆ–é¢„è®¾å€¼ä¸­åŠ è½½
    static const std::vector<std::vector<float>> defaultXCoeffs = {
        // å…‰æ–‘ 1 Xç³»æ•°
        {709.460632f, 11.855237f, -1.977625f, -0.012898f, 0.000192f, 0.012238f, 0.000111f, -0.000002f},
        // å…‰æ–‘ 2 Xç³»æ•°
        {1224.723999f, 11.907899f, -1.564755f, 0.008515f, 0.000191f, 0.012655f, 0.000026f, -0.000001f},
        // å…‰æ–‘ 3 Xç³»æ•°
        {1296.670532f, 11.641463f, -1.451853f, 0.008834f, 0.000329f, 0.008453f, 0.000013f, -0.000002f},
        // å…‰æ–‘ 4 Xç³»æ•°
        {795.380859f, 12.003286f, -1.795737f, -0.032124f, 0.000368f, 0.002715f, 0.000284f, -0.000003f}
    };

    static const std::vector<std::vector<float>> defaultYCoeffs = {
        // å…‰æ–‘ 1 Yç³»æ•°
        {1362.719116f, -0.906065f, -10.206346f, -0.004179f, -0.042488f, 0.002962f, -0.000051f},
        // å…‰æ–‘ 2 Yç³»æ•°
        {1298.638184f, -1.237444f, -10.240284f, -0.004194f, -0.044179f, -0.009231f, -0.000080f},
        // å…‰æ–‘ 3 Yç³»æ•°
        {1909.829224f, -0.751395f, -11.713892f, -0.013533f, -0.011984f, -0.007396f, 0.000063f},
        // å…‰æ–‘ 4 Yç³»æ•°
        {1984.473633f, -0.444348f, -12.664707f, -0.012776f, -0.005881f, -0.001961f, 0.000079f}
    };

    for(int i = 0; i < 4; i++) {
        m_mappingCoefficients[i].xCoeff = defaultXCoeffs[i];
        m_mappingCoefficients[i].yCoeff = defaultYCoeffs[i];
    }
}

void eyeTrack::printceCoefficient(const std::vector<MappingCoefficients> &coeffs, const MappingCoefficients &coeff)
{
    qDebug() << "  Xç³»æ•° (å…±" << coeff.xCoeff.size() << "ä¸ª):";
    for (size_t j = 0; j < coeff.xCoeff.size(); ++j) {
        qDebug() << "    a" << j << ":" << coeff.xCoeff[j];
    }
    qDebug() << "  Yç³»æ•° (å…±" << coeff.yCoeff.size() << "ä¸ª):";
    for (size_t j = 0; j < coeff.yCoeff.size(); ++j) {
        qDebug() << "    b" << j << ":" << coeff.yCoeff[j];
    }

    qDebug() << "æ˜ å°„ç³»æ•°æ•°é‡:" << coeffs.size();

    for (size_t i = 0; i < coeffs.size(); ++i) {
        qDebug() << "æ˜ å°„ç³»æ•°ç»„ #" << i + 1;

        qDebug() << "  Xç³»æ•° (å…±" << coeffs[i].xCoeff.size() << "ä¸ª):";
        for (size_t j = 0; j < coeffs[i].xCoeff.size(); ++j) {
            qDebug() << "    a" << j << ":" << coeffs[i].xCoeff[j];
        }

        qDebug() << "  Yç³»æ•° (å…±" << coeffs[i].yCoeff.size() << "ä¸ª):";
        for (size_t j = 0; j < coeffs[i].yCoeff.size(); ++j) {
            qDebug() << "    b" << j << ":" << coeffs[i].yCoeff[j];
        }
    }
}
void eyeTrack::on_StopPushButton_clicked()
{
    static bool stopFlag = 1;
    if(stopFlag)
    {
        m_stopButton->setText("æ¢å¤");
        pip->pausePipeLine();
    }
    else
    {
        m_stopButton->setText("æš‚åœ");
        pip->resumePipeLine();

    }
    stopFlag = !stopFlag;

}

